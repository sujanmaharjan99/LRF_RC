{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682bd8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/01_Rulo_KGE_vs_lead_dry.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/01_Rulo_KGE_vs_lead_wet.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/02_St_Joseph_KGE_vs_lead_dry.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/02_St_Joseph_KGE_vs_lead_wet.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/03_Kansas_City_KGE_vs_lead_dry.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/03_Kansas_City_KGE_vs_lead_wet.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/04_Waverly_KGE_vs_lead_dry.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/04_Waverly_KGE_vs_lead_wet.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/05_Boonville_KGE_vs_lead_dry.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/05_Boonville_KGE_vs_lead_wet.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/06_Hermann_KGE_vs_lead_dry.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/06_Hermann_KGE_vs_lead_wet.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/07_St_Charles_KGE_vs_lead_dry.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/07_St_Charles_KGE_vs_lead_wet.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/08_Grafton_KGE_vs_lead_dry.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/08_Grafton_KGE_vs_lead_wet.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/09_ST_Louis_KGE_vs_lead_dry.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/09_ST_Louis_KGE_vs_lead_wet.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/10_Chester_KGE_vs_lead_dry.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/10_Chester_KGE_vs_lead_wet.png\n",
      "[WARN] No per-subset CSVs found for 11_Thebes\n",
      "[csv] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage/Seasonal_KGE_stage_by_station_subset_lead.csv\n",
      "Done generating seasonal KGE vs lead time plots for stage.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "########################################\n",
    "# Config (same station order)\n",
    "########################################\n",
    "\n",
    "stations = [\n",
    "    \"01_Rulo\",\n",
    "    \"02_St_Joseph\",\n",
    "    \"03_Kansas_City\",\n",
    "    \"04_Waverly\",\n",
    "    \"05_Boonville\",\n",
    "    \"06_Hermann\",\n",
    "    \"07_St_Charles\",\n",
    "    \"08_Grafton\",\n",
    "    \"09_ST_Louis\",\n",
    "    \"10_Chester\",\n",
    "    \"11_Thebes\"\n",
    "]\n",
    "\n",
    "# This is where your merged per-subset station CSVs live\n",
    "# like: 01_Rulo_mem1_q_tolerance_25.0_day_bracket_730.csv\n",
    "BASE_DIR = Path(\"/media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH\")\n",
    "\n",
    "# Output directory for KGE seasonal plots and CSV\n",
    "OUT_DIR = Path(\"/media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_seasonal_stage\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "########################################\n",
    "# Seasonal logic and utilities\n",
    "########################################\n",
    "\n",
    "def season_from_month(ts):\n",
    "    \"\"\"Return 'dry' or 'wet' based on month of timestamp.\"\"\"\n",
    "    m = ts.month\n",
    "    # dry: Sep(9) Oct(10) Nov(11) Dec(12) Jan(1) Feb(2)\n",
    "    # wet: Mar(3) .. Aug(8)\n",
    "    # Note: Your previous code only had Jan in dry. I'm extending to Feb,\n",
    "    # because otherwise Feb goes nowhere. Adjust if you don't want Feb dry.\n",
    "    if m in [8,9,10,11,12,1,2]:\n",
    "        return \"dry\"\n",
    "    else:\n",
    "        return \"wet\"\n",
    "\n",
    "def get_horizons(cols):\n",
    "    \"\"\"\n",
    "    Given df.columns, find forecast lead columns,\n",
    "    which look like '006','012',...,'720'.\n",
    "    Return only 6h steps from 6h through 720h,\n",
    "    plus 720 if missing in the step sequence.\n",
    "    \"\"\"\n",
    "    horizons_all = sorted(\n",
    "        [c for c in cols if re.fullmatch(r\"\\d{3}\", c)],\n",
    "        key=lambda s: int(s)\n",
    "    )\n",
    "    wanted = []\n",
    "    for h in range(6, 721, 6):\n",
    "        h_str = f\"{h:03d}\"\n",
    "        if h_str in horizons_all:\n",
    "            wanted.append(h_str)\n",
    "    if \"720\" in horizons_all and \"720\" not in wanted:\n",
    "        wanted.append(\"720\")\n",
    "    return wanted\n",
    "\n",
    "def kge(sim, obs):\n",
    "    \"\"\"\n",
    "    Kling-Gupta Efficiency.\n",
    "    \"\"\"\n",
    "    sim = np.asarray(sim, dtype=float)\n",
    "    obs = np.asarray(obs, dtype=float)\n",
    "\n",
    "    mask = np.isfinite(sim) & np.isfinite(obs)\n",
    "    if mask.sum() < 3:\n",
    "        return np.nan\n",
    "\n",
    "    sim = sim[mask]\n",
    "    obs = obs[mask]\n",
    "\n",
    "    r = np.corrcoef(sim, obs)[0, 1]\n",
    "    alpha = np.std(sim) / np.std(obs) if np.std(obs) != 0 else np.nan\n",
    "    beta = np.mean(sim) / np.mean(obs) if np.mean(obs) != 0 else np.nan\n",
    "\n",
    "    return 1 - np.sqrt((r - 1) ** 2 + (alpha - 1) ** 2 + (beta - 1) ** 2)\n",
    "\n",
    "def subset_pretty_name(raw_subset):\n",
    "    \"\"\"\n",
    "    Map folder/subset token to publication-ready label.\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        \"All\": \"Ensemble mean\",\n",
    "        \"mem1\": \"Ensemble 1\",\n",
    "        \"mem2\": \"Ensemble 2\",\n",
    "        \"mem3\": \"Ensemble 3\",\n",
    "        \"mem4\": \"Ensemble 4\",\n",
    "    }\n",
    "    return mapping.get(raw_subset, raw_subset)\n",
    "\n",
    "########################################\n",
    "# Core processing\n",
    "########################################\n",
    "\n",
    "all_kge_rows = []  # to build a big CSV of seasonal KGE\n",
    "\n",
    "for station_name in stations:\n",
    "    # 1. find all *_q_tolerance_*.csv for this station\n",
    "    station_csvs = list(BASE_DIR.glob(f\"{station_name}_*_q_tolerance_*.csv\"))\n",
    "    if not station_csvs:\n",
    "        print(f\"[WARN] No per-subset CSVs found for {station_name}\")\n",
    "        continue\n",
    "\n",
    "    # read each subset file and prep seasonal data\n",
    "    subset_data = {}  # {subset_pretty: df_with_season_and_stage}\n",
    "    for f in station_csvs:\n",
    "        m = re.search(rf\"^{re.escape(station_name)}_(.+?)_q_tolerance_\", f.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        subset_raw = m.group(1)\n",
    "        nice_subset = subset_pretty_name(subset_raw)\n",
    "\n",
    "        df = pd.read_csv(f, parse_dates=[\"time\"]).set_index(\"time\").sort_index()\n",
    "\n",
    "        # we assume df[\"stage_m_USGS_nearest\"] is observed stage\n",
    "        if \"stage_m_USGS_nearest\" not in df.columns:\n",
    "            print(f\"[WARN] {station_name} {nice_subset} missing stage_m_USGS_nearest, skipping\")\n",
    "            continue\n",
    "\n",
    "        # assign season\n",
    "        df[\"season\"] = [season_from_month(ts) for ts in df.index]\n",
    "\n",
    "        subset_data[nice_subset] = df\n",
    "\n",
    "    if not subset_data:\n",
    "        print(f\"[WARN] No usable subset data for {station_name}\")\n",
    "        continue\n",
    "\n",
    "    # determine horizons that exist across any subset\n",
    "    all_cols = set()\n",
    "    for df in subset_data.values():\n",
    "        all_cols.update(df.columns.tolist())\n",
    "    horizons = get_horizons(list(all_cols))\n",
    "    if not horizons:\n",
    "        print(f\"[WARN] No horizons found for {station_name}\")\n",
    "        continue\n",
    "\n",
    "    # compute KGE per season, per subset, per horizon\n",
    "    # we will collect results into a dict:\n",
    "    # kge_by_season[season] = DataFrame with columns:\n",
    "    #   LeadDays, Subset, KGE\n",
    "    season_list = [\"dry\", \"wet\"]\n",
    "    seasonal_records = {s: [] for s in season_list}\n",
    "\n",
    "    for nice_subset, df in subset_data.items():\n",
    "        obs = df[\"stage_m_USGS_nearest\"]\n",
    "\n",
    "        for h in horizons:\n",
    "            if h not in df.columns:\n",
    "                continue\n",
    "            pred = df[h]\n",
    "\n",
    "            tmp = pd.concat([obs, pred, df[\"season\"]], axis=1)\n",
    "            tmp.columns = [\"obs_stage\", \"pred_stage\", \"season\"]\n",
    "            # split by season\n",
    "            for seas in season_list:\n",
    "                subtmp = tmp[tmp[\"season\"] == seas].dropna()\n",
    "                if subtmp.empty:\n",
    "                    continue\n",
    "\n",
    "                kge_val = kge(subtmp[\"pred_stage\"].values, subtmp[\"obs_stage\"].values)\n",
    "\n",
    "                lead_hours = int(h)\n",
    "                lead_days = lead_hours / 24.0 if lead_hours >= 24 else lead_hours / 24.0\n",
    "\n",
    "                seasonal_records[seas].append({\n",
    "                    \"Station\": station_name,\n",
    "                    \"Subset\": nice_subset,\n",
    "                    \"LeadHours\": lead_hours,\n",
    "                    \"LeadDays\": lead_days,\n",
    "                    \"Season\": seas,\n",
    "                    \"KGE\": kge_val\n",
    "                })\n",
    "\n",
    "    # turn those seasonal records into dfs for plotting and saving\n",
    "    df_dry = pd.DataFrame([r for r in seasonal_records[\"dry\"] if r[\"Station\"] == station_name])\n",
    "    df_wet = pd.DataFrame([r for r in seasonal_records[\"wet\"] if r[\"Station\"] == station_name])\n",
    "\n",
    "    # append to global CSV list\n",
    "    all_kge_rows.extend(seasonal_records[\"dry\"])\n",
    "    all_kge_rows.extend(seasonal_records[\"wet\"])\n",
    "\n",
    "    # plot helper\n",
    "    def plot_kge_scatter(df_season, season_label):\n",
    "        if df_season.empty:\n",
    "            print(f\"[INFO] No {season_label} data to plot for {station_name}\")\n",
    "            return\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(9,5))\n",
    "\n",
    "        for subset_name, dsub in df_season.groupby(\"Subset\"):\n",
    "            ax.scatter(\n",
    "                dsub[\"LeadDays\"],\n",
    "                dsub[\"KGE\"],\n",
    "                s=30,\n",
    "                alpha=0.8,\n",
    "                label=subset_name\n",
    "            )\n",
    "\n",
    "        ax.set_xlabel(\"Lead time (days)\")\n",
    "        ax.set_ylabel(\"KGE (stage)\")\n",
    "        ax.set_title(f\"{station_name}: KGE vs lead time ({season_label} season)\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(title=\"Subset\")\n",
    "        ax.set_xlim(left=-0.5, right=30.5)  # 0 to ~30 days view\n",
    "        ax.set_ylim(0.3, 1)             # standard KGE-ish range, adjust if you want\n",
    "        fig.tight_layout()\n",
    "\n",
    "        out_png = OUT_DIR / f\"{station_name}_KGE_vs_lead_{season_label}.png\"\n",
    "        fig.savefig(out_png, dpi=300)\n",
    "        plt.close(fig)\n",
    "        print(f\"[plot] wrote {out_png}\")\n",
    "\n",
    "    # make seasonal plots\n",
    "    plot_kge_scatter(df_dry, \"dry\")\n",
    "    plot_kge_scatter(df_wet, \"wet\")\n",
    "\n",
    "# build and save combined CSV of all stations / both seasons\n",
    "all_kge_df = pd.DataFrame(all_kge_rows)\n",
    "csv_out = OUT_DIR / \"Seasonal_KGE_stage_by_station_subset_lead.csv\"\n",
    "all_kge_df.to_csv(csv_out, index=False)\n",
    "print(f\"[csv] wrote {csv_out}\")\n",
    "\n",
    "print(\"Done generating seasonal KGE vs lead time plots for stage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2750fced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_616983/239732316.py:8: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  colors = cm.get_cmap(\"tab20\", 12)   # 12 distinct colors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_monthly_stage/01_Rulo_KGE_vs_lead_MONTHLY_MEAN.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_monthly_stage/02_St_Joseph_KGE_vs_lead_MONTHLY_MEAN.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_monthly_stage/03_Kansas_City_KGE_vs_lead_MONTHLY_MEAN.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_monthly_stage/04_Waverly_KGE_vs_lead_MONTHLY_MEAN.png\n",
      "[plot] wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_monthly_stage/05_Boonville_KGE_vs_lead_MONTHLY_MEAN.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 164\u001b[39m\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# group by calendar month\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m month_val, g \u001b[38;5;129;01min\u001b[39;00m tmp.groupby(\u001b[33m\"\u001b[39m\u001b[33mMonth\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m g.shape[\u001b[32m0\u001b[39m] < \u001b[32m3\u001b[39m:\n\u001b[32m    166\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/LRF_RC/lib/python3.12/site-packages/pandas/core/groupby/ops.py:620\u001b[39m, in \u001b[36mBaseGrouper.get_iterator\u001b[39m\u001b[34m(self, data, axis)\u001b[39m\n\u001b[32m    618\u001b[39m splitter = \u001b[38;5;28mself\u001b[39m._get_splitter(data, axis=axis)\n\u001b[32m    619\u001b[39m keys = \u001b[38;5;28mself\u001b[39m.group_keys_seq\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(keys, splitter)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/LRF_RC/lib/python3.12/site-packages/pandas/core/groupby/ops.py:1160\u001b[39m, in \u001b[36mDataSplitter.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1157\u001b[39m starts, ends = lib.generate_slices(\u001b[38;5;28mself\u001b[39m._slabels, \u001b[38;5;28mself\u001b[39m.ngroups)\n\u001b[32m   1159\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m start, end \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(starts, ends):\n\u001b[32m-> \u001b[39m\u001b[32m1160\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._chop(sdata, \u001b[38;5;28mslice\u001b[39m(start, end))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/LRF_RC/lib/python3.12/site-packages/pandas/core/groupby/ops.py:1186\u001b[39m, in \u001b[36mFrameSplitter._chop\u001b[39m\u001b[34m(self, sdata, slice_obj)\u001b[39m\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chop\u001b[39m(\u001b[38;5;28mself\u001b[39m, sdata: DataFrame, slice_obj: \u001b[38;5;28mslice\u001b[39m) -> DataFrame:\n\u001b[32m   1181\u001b[39m     \u001b[38;5;66;03m# Fastpath equivalent to:\u001b[39;00m\n\u001b[32m   1182\u001b[39m     \u001b[38;5;66;03m# if self.axis == 0:\u001b[39;00m\n\u001b[32m   1183\u001b[39m     \u001b[38;5;66;03m#     return sdata.iloc[slice_obj]\u001b[39;00m\n\u001b[32m   1184\u001b[39m     \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[32m   1185\u001b[39m     \u001b[38;5;66;03m#     return sdata.iloc[:, slice_obj]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1186\u001b[39m     mgr = sdata._mgr.get_slice(slice_obj, axis=\u001b[32m1\u001b[39m - \u001b[38;5;28mself\u001b[39m.axis)\n\u001b[32m   1187\u001b[39m     df = sdata._constructor_from_mgr(mgr, axes=mgr.axes)\n\u001b[32m   1188\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df.__finalize__(sdata, method=\u001b[33m\"\u001b[39m\u001b[33mgroupby\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/internals.pyx:871\u001b[39m, in \u001b[36mpandas._libs.internals.BlockManager.get_slice\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/internals.pyx:855\u001b[39m, in \u001b[36mpandas._libs.internals.BlockManager._slice_mgr_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/LRF_RC/lib/python3.12/site-packages/pandas/core/indexes/base.py:5441\u001b[39m, in \u001b[36mIndex._getitem_slice\u001b[39m\u001b[34m(self, slobj)\u001b[39m\n\u001b[32m   5437\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_getitem_slice\u001b[39m(\u001b[38;5;28mself\u001b[39m, slobj: \u001b[38;5;28mslice\u001b[39m) -> Self:\n\u001b[32m   5438\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5439\u001b[39m \u001b[33;03m    Fastpath for __getitem__ when we know we have a slice.\u001b[39;00m\n\u001b[32m   5440\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5441\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._data[slobj]\n\u001b[32m   5442\u001b[39m     result = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)._simple_new(res, name=\u001b[38;5;28mself\u001b[39m._name, refs=\u001b[38;5;28mself\u001b[39m._references)\n\u001b[32m   5443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_engine\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/LRF_RC/lib/python3.12/site-packages/pandas/core/arrays/datetimelike.py:390\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;129m@overload\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    386\u001b[39m     item: SequenceIndexer | PositionalIndexerTuple,\n\u001b[32m    387\u001b[39m ) -> Self:\n\u001b[32m    388\u001b[39m     ...\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: PositionalIndexer2D) -> Self | DTScalarOrNaT:\n\u001b[32m    391\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    392\u001b[39m \u001b[33;03m    This getitem defers to the underlying array, which by-definition can\u001b[39;00m\n\u001b[32m    393\u001b[39m \u001b[33;03m    only handle list-likes, slices, and integer scalars\u001b[39;00m\n\u001b[32m    394\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    395\u001b[39m     \u001b[38;5;66;03m# Use cast as we know we will get back a DatetimeLikeArray or DTScalar,\u001b[39;00m\n\u001b[32m    396\u001b[39m     \u001b[38;5;66;03m# but skip evaluating the Union at runtime for performance\u001b[39;00m\n\u001b[32m    397\u001b[39m     \u001b[38;5;66;03m# (see https://github.com/pandas-dev/pandas/pull/44624)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "colors = cm.get_cmap(\"tab20\", 12)   # 12 distinct colors\n",
    "line_styles = [\"-\", \"--\", \"-.\", \":\", \"-\", \"--\", \"-.\", \":\", \"-\", \"--\", \"-.\", \":\"]\n",
    "\n",
    "########################################\n",
    "# Config (same station order)\n",
    "########################################\n",
    "\n",
    "stations = [\n",
    "    \"01_Rulo\",\n",
    "    \"02_St_Joseph\",\n",
    "    \"03_Kansas_City\",\n",
    "    \"04_Waverly\",\n",
    "    \"05_Boonville\",\n",
    "    \"06_Hermann\",\n",
    "    \"07_St_Charles\",\n",
    "    \"08_Grafton\",\n",
    "    \"09_ST_Louis\",\n",
    "    \"10_Chester\",\n",
    "    \"11_Thebes\"\n",
    "]\n",
    "\n",
    "# This is where your merged per-subset station CSVs live\n",
    "# like: 01_Rulo_mem1_q_tolerance_25.0_day_bracket_730.csv\n",
    "BASE_DIR = Path(\"/media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH\")\n",
    "\n",
    "# Output directory for KGE monthly plots and CSV\n",
    "OUT_DIR = Path(\"/media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/KGE_monthly_stage\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "########################################\n",
    "# Utilities\n",
    "########################################\n",
    "\n",
    "def get_horizons(cols):\n",
    "    \"\"\"\n",
    "    Given df.columns, find forecast lead columns,\n",
    "    which look like '006','012',...,'720'.\n",
    "    Return only 6 h steps from 6 h through 720 h,\n",
    "    plus 720 if missing in the step sequence.\n",
    "    \"\"\"\n",
    "    horizons_all = sorted(\n",
    "        [c for c in cols if re.fullmatch(r\"\\d{3}\", c)],\n",
    "        key=lambda s: int(s)\n",
    "    )\n",
    "    wanted = []\n",
    "    for h in range(6, 721, 6):\n",
    "        h_str = f\"{h:03d}\"\n",
    "        if h_str in horizons_all:\n",
    "            wanted.append(h_str)\n",
    "    if \"720\" in horizons_all and \"720\" not in wanted:\n",
    "        wanted.append(\"720\")\n",
    "    return wanted\n",
    "\n",
    "def kge(sim, obs):\n",
    "    \"\"\"\n",
    "    Kling-Gupta Efficiency.\n",
    "    \"\"\"\n",
    "    sim = np.asarray(sim, dtype=float)\n",
    "    obs = np.asarray(obs, dtype=float)\n",
    "\n",
    "    mask = np.isfinite(sim) & np.isfinite(obs)\n",
    "    if mask.sum() < 3:\n",
    "        return np.nan\n",
    "\n",
    "    sim = sim[mask]\n",
    "    obs = obs[mask]\n",
    "\n",
    "    r = np.corrcoef(sim, obs)[0, 1]\n",
    "    alpha = np.std(sim) / np.std(obs) if np.std(obs) != 0 else np.nan\n",
    "    beta = np.mean(sim) / np.mean(obs) if np.mean(obs) != 0 else np.nan\n",
    "\n",
    "    return 1 - np.sqrt((r - 1) ** 2 + (alpha - 1) ** 2 + (beta - 1) ** 2)\n",
    "\n",
    "def subset_pretty_name(raw_subset):\n",
    "    \"\"\"\n",
    "    Map folder/subset token to publication-ready label.\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        \"All\": \"Ensemble mean\"\n",
    "        #\"mem1\": \"Ensemble 1\",\n",
    "        #\"mem2\": \"Ensemble 2\",\n",
    "        #\"mem3\": \"Ensemble 3\",\n",
    "        #\"mem4\": \"Ensemble 4\",\n",
    "    }\n",
    "    return mapping.get(raw_subset, raw_subset)\n",
    "\n",
    "########################################\n",
    "# Core processing (monthly KGE)\n",
    "########################################\n",
    "\n",
    "all_kge_rows = []  # to build a big CSV of monthly KGE\n",
    "\n",
    "for station_name in stations:\n",
    "    # 1. find all *_q_tolerance_*.csv for this station\n",
    "    station_csvs = list(BASE_DIR.glob(f\"{station_name}_*_q_tolerance_*.csv\"))\n",
    "    if not station_csvs:\n",
    "        print(f\"[WARN] No per-subset CSVs found for {station_name}\")\n",
    "        continue\n",
    "\n",
    "    # read each subset file and prep data\n",
    "    subset_data = {}  # {subset_pretty: df}\n",
    "    for f in station_csvs:\n",
    "        m = re.search(rf\"^{re.escape(station_name)}_(.+?)_q_tolerance_\", f.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        subset_raw = m.group(1)\n",
    "        nice_subset = subset_pretty_name(subset_raw)\n",
    "\n",
    "        df = pd.read_csv(f, parse_dates=[\"time\"]).set_index(\"time\").sort_index()\n",
    "\n",
    "        # we assume df[\"stage_m_USGS_nearest\"] is observed stage\n",
    "        if \"stage_m_USGS_nearest\" not in df.columns:\n",
    "            print(f\"[WARN] {station_name} {nice_subset} missing stage_m_USGS_nearest, skipping\")\n",
    "            continue\n",
    "\n",
    "        # add month info\n",
    "        df[\"Month\"] = df.index.month\n",
    "        df[\"MonthName\"] = df.index.strftime(\"%b\")  # Jan, Feb, ...\n",
    "\n",
    "        subset_data[nice_subset] = df\n",
    "\n",
    "    if not subset_data:\n",
    "        print(f\"[WARN] No usable subset data for {station_name}\")\n",
    "        continue\n",
    "\n",
    "    # determine horizons that exist across any subset\n",
    "    all_cols = set()\n",
    "    for df in subset_data.values():\n",
    "        all_cols.update(df.columns.tolist())\n",
    "    horizons = get_horizons(list(all_cols))\n",
    "    if not horizons:\n",
    "        print(f\"[WARN] No horizons found for {station_name}\")\n",
    "        continue\n",
    "\n",
    "    # collect monthly records for this station\n",
    "    station_monthly_records = []\n",
    "\n",
    "    for nice_subset, df in subset_data.items():\n",
    "        obs = df[\"stage_m_USGS_nearest\"]\n",
    "\n",
    "        for h in horizons:\n",
    "            if h not in df.columns:\n",
    "                continue\n",
    "            pred = df[h]\n",
    "\n",
    "            tmp = pd.concat(\n",
    "                [obs, pred, df[\"Month\"], df[\"MonthName\"]],\n",
    "                axis=1\n",
    "            )\n",
    "            tmp.columns = [\"obs_stage\", \"pred_stage\", \"Month\", \"MonthName\"]\n",
    "            tmp = tmp.dropna()\n",
    "\n",
    "            if tmp.empty:\n",
    "                continue\n",
    "\n",
    "            # group by calendar month\n",
    "            for month_val, g in tmp.groupby(\"Month\"):\n",
    "                if g.shape[0] < 3:\n",
    "                    continue\n",
    "\n",
    "                kge_val = kge(g[\"pred_stage\"].values, g[\"obs_stage\"].values)\n",
    "\n",
    "                lead_hours = int(h)\n",
    "                lead_days = lead_hours / 24.0\n",
    "\n",
    "                rec = {\n",
    "                    \"Station\": station_name,\n",
    "                    \"Subset\": nice_subset,\n",
    "                    \"LeadHours\": lead_hours,\n",
    "                    \"LeadDays\": lead_days,\n",
    "                    \"Month\": int(month_val),\n",
    "                    \"MonthName\": g[\"MonthName\"].iloc[0],\n",
    "                    \"KGE\": kge_val,\n",
    "                }\n",
    "\n",
    "                station_monthly_records.append(rec)\n",
    "                all_kge_rows.append(rec)\n",
    "\n",
    "    # plotting for this station, month by month\n",
    "    df_station_month = pd.DataFrame(station_monthly_records)\n",
    "    if df_station_month.empty:\n",
    "        print(f\"[INFO] No monthly KGE data to plot for {station_name}\")\n",
    "        continue\n",
    "\n",
    "    def plot_kge_monthly_lines(df_station):\n",
    "        # df_station has rows for one station with fields:\n",
    "        # Station, Subset, LeadDays, Month, MonthName, KGE\n",
    "\n",
    "        # Create consistent month order\n",
    "        month_order = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "\n",
    "        # Build figure\n",
    "        fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "        # Loop months in calendar order\n",
    "        for m in month_order:\n",
    "            df_m = df_station[df_station[\"Month\"] == m]\n",
    "            if df_m.empty:\n",
    "                continue\n",
    "\n",
    "            # Multiple subsets: draw one line per subset for this month\n",
    "            for subset_name, dsub in df_m.groupby(\"Subset\"):\n",
    "                dsub_sorted = dsub.sort_values(\"LeadDays\")\n",
    "                ax.plot(\n",
    "                df_m[\"LeadDays\"],\n",
    "                df_m[\"KGE\"],\n",
    "                linewidth=2,\n",
    "                linestyle=line_styles[m-1],\n",
    "                label=df_m[\"MonthName\"].iloc[0]\n",
    "            )\n",
    "\n",
    "\n",
    "        ax.set_xlabel(\"Lead time (days)\")\n",
    "        ax.set_ylabel(\"KGE (stage)\")\n",
    "        ax.set_title(f\"{station_name}: Monthly KGE vs lead time\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlim(-0.2, 30.2)\n",
    "        ax.set_ylim(0.3, 1.0)\n",
    "\n",
    "        # Avoid duplicate legend labels\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        unique = dict(zip(labels, handles))\n",
    "        ax.legend(unique.values(), unique.keys(), title=\"Month - Subset\", ncol=2)\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        out_png = OUT_DIR / f\"{station_name}_KGE_vs_lead_MONTHLY_LINES.png\"\n",
    "        fig.savefig(out_png, dpi=300)\n",
    "        plt.close(fig)\n",
    "        print(f\"[plot] wrote {out_png}\")\n",
    "\n",
    "    def plot_kge_monthly_mean(df_station):\n",
    "        # df_station has columns:\n",
    "        # Station, Subset, LeadDays, Month, MonthName, KGE\n",
    "\n",
    "        # Compute mean KGE across subsets for each month and lead time\n",
    "        df_mean = (\n",
    "            df_station\n",
    "            .groupby([\"Month\", \"MonthName\", \"LeadDays\"], as_index=False)\n",
    "            .agg({\"KGE\": \"mean\"})\n",
    "        )\n",
    "\n",
    "        month_order = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "        for m in month_order:\n",
    "            df_m = df_mean[df_mean[\"Month\"] == m].sort_values(\"LeadDays\")\n",
    "            if df_m.empty:\n",
    "                continue\n",
    "\n",
    "            ax.plot(\n",
    "                df_m[\"LeadDays\"],\n",
    "                df_m[\"KGE\"],\n",
    "                linewidth=2,\n",
    "                alpha=0.9,\n",
    "                color=colors(m-1),   # month 1â†’index 0\n",
    "                label=df_m[\"MonthName\"].iloc[0],\n",
    "            )\n",
    "\n",
    "\n",
    "        ax.set_xlabel(\"Lead time (days)\")\n",
    "        ax.set_ylabel(\"KGE (stage)\")\n",
    "        ax.set_title(f\"{station_name}: Monthly mean KGE vs lead time\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlim(-0.2, 30.2)\n",
    "        ax.set_ylim(0.1, 1.0)\n",
    "\n",
    "        ax.legend(title=\"Month\", ncol=3)\n",
    "        fig.tight_layout()\n",
    "\n",
    "        out_png = OUT_DIR / f\"{station_name}_KGE_vs_lead_MONTHLY_MEAN.png\"\n",
    "        fig.savefig(out_png, dpi=300)\n",
    "        plt.close(fig)\n",
    "        print(f\"[plot] wrote {out_png}\")\n",
    "        \n",
    "    plot_kge_monthly_mean(df_station_month)\n",
    "\n",
    "\n",
    "#plot_kge_monthly_lines(df_station_month)\n",
    "\n",
    "# build and save combined CSV of all stations (monthly)\n",
    "all_kge_df = pd.DataFrame(all_kge_rows)\n",
    "csv_out = OUT_DIR / \"Monthly_KGE_stage_by_station_subset_lead.csv\"\n",
    "all_kge_df.to_csv(csv_out, index=False)\n",
    "print(f\"[csv] wrote {csv_out}\")\n",
    "\n",
    "print(\"Done generating monthly KGE vs lead time plots for stage.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b548e",
   "metadata": {},
   "source": [
    "<h1>box plot</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "050712c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/01_Rulo_errors_boxplot_dry_season.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/01_Rulo_errors_boxplot_wet_season.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/02_St_Joseph_errors_boxplot_dry_season.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/02_St_Joseph_errors_boxplot_wet_season.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/03_Kansas_City_errors_boxplot_dry_season.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/03_Kansas_City_errors_boxplot_wet_season.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/04_Waverly_errors_boxplot_dry_season.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/04_Waverly_errors_boxplot_wet_season.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/05_Boonville_errors_boxplot_dry_season.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/05_Boonville_errors_boxplot_wet_season.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/06_Hermann_errors_boxplot_dry_season.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/06_Hermann_errors_boxplot_wet_season.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/07_St_Charles_errors_boxplot_dry_season.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/07_St_Charles_errors_boxplot_wet_season.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/08_Grafton_errors_boxplot_dry_season.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/08_Grafton_errors_boxplot_wet_season.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/09_ST_Louis_errors_boxplot_dry_season.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/09_ST_Louis_errors_boxplot_wet_season.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/10_Chester_errors_boxplot_dry_season.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/10_Chester_errors_boxplot_wet_season.png\n",
      "[WARN] no seasonal CSVs found for 11_Thebes\n",
      "Seasonal dry/wet error boxplots generated for all stations.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import os\n",
    "\n",
    "# stations info (same order as before)\n",
    "stations = [\n",
    "    \"01_Rulo\",\n",
    "    \"02_St_Joseph\",\n",
    "    \"03_Kansas_City\",\n",
    "    \"04_Waverly\",\n",
    "    \"05_Boonville\",\n",
    "    \"06_Hermann\",\n",
    "    \"07_St_Charles\",\n",
    "    \"08_Grafton\",\n",
    "    \"09_ST_Louis\",\n",
    "    \"10_Chester\",\n",
    "    \"11_Thebes\"\n",
    "]\n",
    "\n",
    "out_dir = Path(\"/media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH\")\n",
    "\n",
    "# helper: assign season label based on timestamp month\n",
    "def season_from_month(ts):\n",
    "    m = ts.month\n",
    "    # dry: Sep(9) Oct(10) Nov(11) Dec(12) Jan(1)\n",
    "    # wet: Mar(3) ... Aug(8)\n",
    "    if m in [9,10,11,12,1,2]:\n",
    "        return \"dry\"\n",
    "    else:\n",
    "        return \"wet\"\n",
    "\n",
    "# horizons to include in boxplots:\n",
    "# every 24h step starting at 6h up to 720h, plus 720 if missing.\n",
    "def get_horizons(cols):\n",
    "    # cols is list of strings like [\"006\",\"012\",\"018\",...]\n",
    "    horizons_all = sorted([c for c in cols if re.fullmatch(r\"\\d{3}\", c)], key=lambda s:int(s))\n",
    "    wanted = []\n",
    "    for h in range(6, 721, 24):\n",
    "        h_str = f\"{h:03d}\"\n",
    "        if h_str in horizons_all:\n",
    "            wanted.append(h_str)\n",
    "    if \"720\" in horizons_all and \"720\" not in wanted:\n",
    "        wanted.append(\"720\")\n",
    "    return wanted\n",
    "\n",
    "# plotting function:\n",
    "def plot_season_boxplots(station_name):\n",
    "    \"\"\"\n",
    "    For one station, load the per-subfolder merged CSVs that were already written:\n",
    "      {station}_{subset}_q_tolerance_...csv\n",
    "    We'll search for those in out_dir.\n",
    "    We'll build seasonal error arrays for each subset and horizon.\n",
    "    Then make 2 figures: dry, wet.\n",
    "    Each figure has x-axis = lead time (days), and for each lead time,\n",
    "    we overlay 5 boxplots side by side? or combine?\n",
    "    \n",
    "    You asked: \"I think for the box plot it would only have 2 figures per station. dry and wet and each one having 30 days error box plot\"\n",
    "    \n",
    "    We'll do grouped boxplots per lead time where each subset is a separate color at that lead.\n",
    "    That way each figure shows mem1, mem2, mem3, mem4, Ensemble mean together.\n",
    "    \"\"\"\n",
    "\n",
    "    # find all CSVs for this station (the merged outputs from earlier)\n",
    "    # pattern example: \"01_Rulo_mem1_q_tolerance_25.0_day_bracket_730.csv\"\n",
    "    station_csvs = list(out_dir.glob(f\"{station_name}_*_q_tolerance_*.csv\"))\n",
    "    if not station_csvs:\n",
    "        print(f\"[WARN] no seasonal CSVs found for {station_name}\")\n",
    "        return\n",
    "\n",
    "    # read each subset file into dict {subset_name: df}\n",
    "    subset_dfs = {}\n",
    "    for f in station_csvs:\n",
    "        # subset is between station_ and _q_tolerance\n",
    "        # e.g. \"01_Rulo_mem1_q_tolerance_25.0_day_bracket_730.csv\"\n",
    "        m = re.search(rf\"^{re.escape(station_name)}_(.+?)_q_tolerance_\", f.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        subset_raw = m.group(1)  # e.g. \"mem1\" or \"All\"\n",
    "        subset_name = {\n",
    "            \"All\": \"Ensemble mean\",\n",
    "            \"mem1\": \"Ensemble 1\",\n",
    "            \"mem2\": \"Ensemble 2\",\n",
    "            \"mem3\": \"Ensemble 3\",\n",
    "            \"mem4\": \"Ensemble 4\",\n",
    "        }.get(subset_raw, subset_raw)\n",
    "\n",
    "        df = pd.read_csv(f, parse_dates=[\"time\"])\n",
    "        df = df.set_index(\"time\").sort_index()\n",
    "\n",
    "        # add season column\n",
    "        df[\"season\"] = [season_from_month(ts) for ts in df.index]\n",
    "\n",
    "        subset_dfs[subset_name] = df\n",
    "\n",
    "    if not subset_dfs:\n",
    "        print(f\"[WARN] could not parse subset dfs for {station_name}\")\n",
    "        return\n",
    "\n",
    "    # figure out consistent horizons across subsets\n",
    "    all_cols = set()\n",
    "    for df in subset_dfs.values():\n",
    "        all_cols.update(df.columns.tolist())\n",
    "    all_cols = list(all_cols)\n",
    "    horizons = get_horizons(all_cols)\n",
    "    if not horizons:\n",
    "        print(f\"[WARN] no horizons for {station_name}\")\n",
    "        return\n",
    "\n",
    "    # we will build data structure:\n",
    "    # errors[season][horizon][subset] = array of (forecast - obs)\n",
    "    seasons = [\"dry\",\"wet\"]\n",
    "    errors = {s: {h: {} for h in horizons} for s in seasons}\n",
    "\n",
    "    for subset_name, df in subset_dfs.items():\n",
    "        if \"stage_m_USGS_nearest\" not in df.columns:\n",
    "            print(f\"[WARN] {station_name} {subset_name} missing stage_m_USGS_nearest\")\n",
    "            continue\n",
    "        obs = df[\"stage_m_USGS_nearest\"]\n",
    "\n",
    "        for h in horizons:\n",
    "            if h not in df.columns:\n",
    "                continue\n",
    "            pred = df[h]\n",
    "            err_series = (pred - obs).dropna()\n",
    "\n",
    "            if err_series.empty:\n",
    "                continue\n",
    "\n",
    "            # split by season\n",
    "            joined = pd.concat([err_series, df[\"season\"]], axis=1)\n",
    "            joined.columns = [\"err\",\"season\"]\n",
    "            for s in seasons:\n",
    "                vals = joined.loc[joined[\"season\"] == s, \"err\"].dropna().values\n",
    "                if vals.size == 0:\n",
    "                    continue\n",
    "                errors[s][h].setdefault(subset_name, [])\n",
    "                errors[s][h][subset_name].extend(vals.tolist())\n",
    "\n",
    "    # Now we plot two figs: dry and wet\n",
    "    for s in seasons:\n",
    "        # Build grouped boxplot data.\n",
    "        # For each horizon h in horizons, we will have up to 5 subsets.\n",
    "        subset_order = [\"Ensemble mean\",\"Ensemble 1\",\"Ensemble 2\",\"Ensemble 3\",\"Ensemble 4\"]\n",
    "        # Collect data in consistent order\n",
    "        box_data = []      # list of lists of arrays, shape ~ (len(horizons)*len(subset_order))\n",
    "        box_positions = [] # x positions for each box\n",
    "        box_labels_days = []  # one label per horizon group (days)\n",
    "        group_width = 0.8\n",
    "        n_sub = len(subset_order)\n",
    "        if n_sub == 0:\n",
    "            continue\n",
    "        dx = group_width / max(n_sub,1)\n",
    "\n",
    "        xpos = 1\n",
    "        for h in horizons:\n",
    "            # lead time label in days\n",
    "            lead_hours = int(h)\n",
    "            if lead_hours == 6:\n",
    "                day_label = \"6 h\"\n",
    "            else:\n",
    "                day_label = f\"{lead_hours//24:d}\"\n",
    "\n",
    "            # stash label for tick location (center of group)\n",
    "            box_labels_days.append(day_label)\n",
    "\n",
    "            # for each subset in fixed order, append its error array or empty\n",
    "            for si, subset_name in enumerate(subset_order):\n",
    "                vals = errors[s][h].get(subset_name, [])\n",
    "                box_data.append(vals if len(vals)>0 else [np.nan])\n",
    "                # position offset within group\n",
    "                box_positions.append(xpos + (si - (n_sub-1)/2.0)*dx)\n",
    "\n",
    "            xpos += 1  # next horizon group\n",
    "\n",
    "        if not box_data:\n",
    "            print(f\"[INFO] No data to plot for {station_name} season={s}\")\n",
    "            continue\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(14,6))\n",
    "\n",
    "        bp = ax.boxplot(\n",
    "            box_data,\n",
    "            positions=box_positions,\n",
    "            widths=dx*0.9,\n",
    "            showfliers=False,\n",
    "            showmeans=True,\n",
    "            meanline=True,\n",
    "            meanprops={\"marker\": \"D\", \"markersize\": 2},\n",
    "            medianprops={\"linewidth\":1.2, \"color\":\"blue\"},\n",
    "            patch_artist=True\n",
    "        )\n",
    "\n",
    "        # color each subset consistently across horizons\n",
    "        # we'll just rotate through 5 default matplotlib colors by subset index\n",
    "        # (not manually setting custom colors, to keep matplotlib defaults valid if you change style)\n",
    "        # assign facecolors subset-wise:\n",
    "        import itertools\n",
    "        color_cycle = plt.rcParams['axes.prop_cycle'].by_key().get('color', [])\n",
    "        # build map subset -> color index\n",
    "        subset_color = {subset_name: color_cycle[i % len(color_cycle)]\n",
    "                        for i, subset_name in enumerate(subset_order)}\n",
    "        for i_box, patch in enumerate(bp['boxes']):\n",
    "            subset_idx = i_box % n_sub\n",
    "            subset_name = subset_order[subset_idx]\n",
    "            patch.set_facecolor(subset_color.get(subset_name, \"#cccccc\"))\n",
    "            patch.set_alpha(0.6)\n",
    "\n",
    "        # x ticks at the center of each horizon group\n",
    "        tick_positions = []\n",
    "        xpos = 1\n",
    "        for _h in horizons:\n",
    "            tick_positions.append(xpos)\n",
    "            xpos += 1\n",
    "\n",
    "        ax.set_xticks(tick_positions)\n",
    "        ax.set_xticklabels(box_labels_days, rotation=0)\n",
    "        ax.axhline(0, linestyle=\"--\", linewidth=1.2, color=\"k\", alpha=0.8, zorder=0)\n",
    "        ax.set_ylabel(\"Gage height forecast error (m)\")\n",
    "        if s == \"dry\":\n",
    "            season_title = \"Dry season (Sep-Feb)\"\n",
    "        else:\n",
    "            season_title = \"Wet season (Mar-Aug)\"\n",
    "\n",
    "        ax.set_xlabel(\"Lead time (days)\")\n",
    "        ax.set_title(f\"{station_name}: Stage error by lead time, {season_title}\")\n",
    "        ax.grid(axis=\"y\", alpha=0.3)\n",
    "        ax.set_ylim(-6, 4)\n",
    "        # legend from subset_color\n",
    "        handles = []\n",
    "        labels  = []\n",
    "        for sub_name in subset_order:\n",
    "            if any(sub_name in d for d in errors[s].values()):\n",
    "                handles.append(plt.Line2D([0],[0], marker='s', linestyle='',\n",
    "                              markerfacecolor=subset_color.get(sub_name,\"#ccc\"),\n",
    "                              markeredgecolor='k', alpha=0.6))\n",
    "                labels.append(sub_name)\n",
    "        ax.legend(handles, labels, title=\"Subset\", ncol=2, loc=\"upper left\", frameon=False)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        out_png = out_dir / f\"{station_name}_errors_boxplot_{s}_season.png\"\n",
    "        fig.savefig(out_png, dpi=300)\n",
    "        plt.close(fig)\n",
    "        print(f\"Wrote {out_png}\")\n",
    "\n",
    "# run for all stations\n",
    "for st in stations:\n",
    "    plot_season_boxplots(st)\n",
    "\n",
    "print(\"Seasonal dry/wet error boxplots generated for all stations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6933a761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing season: dry\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/lag_corr_matrix_STL_vs_CHS_dry.csv\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/lag_corr_heatmap_corr_dry.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/lag_corr_heatmap_lagdays_dry.png\n",
      "Processing season: wet\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/lag_corr_matrix_STL_vs_CHS_wet.csv\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/lag_corr_heatmap_corr_wet.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/lag_corr_heatmap_lagdays_wet.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "########################################\n",
    "# User config\n",
    "########################################\n",
    "\n",
    "out_dir = Path(\"/media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH\")\n",
    "\n",
    "stl_file     = out_dir / \"09_ST_Louis_All_q_tolerance_25.0_day_bracket_1000.csv\"\n",
    "chester_file = out_dir / \"10_Chester_All_q_tolerance_25.0_day_bracket_1000.csv\"\n",
    "\n",
    "obs_col  = \"stage_m_USGS_nearest\"\n",
    "time_col = \"time\"\n",
    "\n",
    "# Lead times (forecast horizons) to include in hours\n",
    "stl_horizons_hours = [6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72]\n",
    "chs_horizons_hours = [6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72, 78, 84, 90, 96, 102, 108, 114, 120]\n",
    "\n",
    "# Lag sweep settings\n",
    "max_lag_days   = 5        # we'll look +/- 5 days time shift\n",
    "lag_step_hours = 6        # check lag every 6 hours\n",
    "########################################\n",
    "\n",
    "\n",
    "def tag_season(ts):\n",
    "    \"\"\"Return 'dry' or 'wet' according to your rule:\n",
    "       dry = Sep(9), Oct(10), Nov(11), Dec(12), Jan(1)\n",
    "       wet = Mar(3)...Aug(8)\n",
    "       Feb(2) is not mentioned in your rule. We have to assign it.\n",
    "       I'll treat Feb as 'dry' since it's low-flow/cold season for most midwest rivers.\n",
    "    \"\"\"\n",
    "    m = ts.month\n",
    "    if m in [9, 10, 11, 12, 1, 2]:\n",
    "        return \"dry\"\n",
    "    else:\n",
    "        return \"wet\"\n",
    "\n",
    "\n",
    "def pearson_r(x, y):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    if x.size < 2 or y.size < 2:\n",
    "        return np.nan\n",
    "    return float(np.corrcoef(x, y)[0, 1])\n",
    "\n",
    "\n",
    "def lagged_corr(up_series, down_series, max_lag_days, lag_step_hours):\n",
    "    \"\"\"\n",
    "    up_series:   upstream error series (St. Louis)\n",
    "    down_series: downstream error series (Chester)\n",
    "\n",
    "    We shift 'down_series' in time by lag_h hours.\n",
    "    Positive lag means Chester is moved forward in time (arrives later).\n",
    "    Returns DataFrame of [lag_hours, corr, N].\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    max_lag_hours = max_lag_days * 24\n",
    "    lag_hours_list = np.arange(-max_lag_hours,\n",
    "                               max_lag_hours + 0.1,\n",
    "                               lag_step_hours)\n",
    "    for lag_h in lag_hours_list:\n",
    "        shifted = down_series.copy()\n",
    "        shifted.index = shifted.index + pd.Timedelta(hours=lag_h)\n",
    "\n",
    "        merged = pd.concat(\n",
    "            [up_series.rename(\"up\"),\n",
    "             shifted.rename(\"down\")],\n",
    "            axis=1\n",
    "        ).dropna()\n",
    "\n",
    "        if merged.empty:\n",
    "            results.append({\"lag_hours\": lag_h, \"corr\": np.nan, \"N\": 0})\n",
    "        else:\n",
    "            r = pearson_r(merged[\"up\"].values, merged[\"down\"].values)\n",
    "            results.append({\"lag_hours\": lag_h, \"corr\": r, \"N\": len(merged)})\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def best_lag_info(lag_df):\n",
    "    \"\"\"Given lag_df from lagged_corr, return (best_lag_hours, best_corr, N_at_best).\"\"\"\n",
    "    lag_df_valid = lag_df.dropna(subset=[\"corr\"])\n",
    "    if lag_df_valid.empty:\n",
    "        return np.nan, np.nan, 0\n",
    "    idx_best = lag_df_valid[\"corr\"].idxmax()\n",
    "    row = lag_df_valid.loc[idx_best]\n",
    "    return float(row[\"lag_hours\"]), float(row[\"corr\"]), int(row[\"N\"])\n",
    "\n",
    "\n",
    "def load_station_df(csv_path, obs_col, time_col):\n",
    "    \"\"\"Load CSV, set DatetimeIndex, add season label, return DataFrame.\"\"\"\n",
    "    df = pd.read_csv(csv_path, parse_dates=[time_col]).set_index(time_col).sort_index()\n",
    "    df[\"season\"] = [tag_season(ts) for ts in df.index]\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_season_matrix(stl_df, chs_df, stl_horizons_hours, chs_horizons_hours,\n",
    "                          season_label, max_lag_days, lag_step_hours, obs_col):\n",
    "    \"\"\"\n",
    "    For a given season ('dry' or 'wet'), build:\n",
    "      1) correlation matrix (max corr at best lag)\n",
    "      2) lag matrix (best lag in days at that max corr)\n",
    "\n",
    "    Returns (result_df, heat_corr, heat_lag)\n",
    "    where:\n",
    "      - result_df is long form rows for CSV\n",
    "      - heat_corr is a 2D DataFrame indexed by STL horizon hr, cols=CHS horizon hr\n",
    "      - heat_lag is same shape but best lag in days\n",
    "    \"\"\"\n",
    "    recs = []\n",
    "\n",
    "    # convenience subsets for that season only\n",
    "    stl_season = stl_df[stl_df[\"season\"] == season_label]\n",
    "    chs_season = chs_df[chs_df[\"season\"] == season_label]\n",
    "\n",
    "    for h_up in stl_horizons_hours:\n",
    "        col_up = f\"{h_up:03d}\"\n",
    "        if col_up not in stl_season.columns:\n",
    "            continue\n",
    "\n",
    "        stl_err = (stl_season[col_up] - stl_season[obs_col]).dropna().sort_index()\n",
    "        if stl_err.empty:\n",
    "            continue\n",
    "\n",
    "        for h_down in chs_horizons_hours:\n",
    "            col_down = f\"{h_down:03d}\"\n",
    "            if col_down not in chs_season.columns:\n",
    "                continue\n",
    "\n",
    "            chs_err = (chs_season[col_down] - chs_season[obs_col]).dropna().sort_index()\n",
    "            if chs_err.empty:\n",
    "                continue\n",
    "\n",
    "            # compute lag curve for this horizon pair\n",
    "            lag_df = lagged_corr(\n",
    "                up_series=stl_err,\n",
    "                down_series=chs_err,\n",
    "                max_lag_days=max_lag_days,\n",
    "                lag_step_hours=lag_step_hours\n",
    "            )\n",
    "\n",
    "            lag_hours, max_corr, n_pairs = best_lag_info(lag_df)\n",
    "\n",
    "            recs.append({\n",
    "                \"season\": season_label,\n",
    "                \"STL_horizon_hr\": h_up,\n",
    "                \"CHS_horizon_hr\": h_down,\n",
    "                \"best_lag_hours\": lag_hours,\n",
    "                \"best_lag_days\": (lag_hours / 24.0) if not np.isnan(lag_hours) else np.nan,\n",
    "                \"max_corr\": max_corr,\n",
    "                \"N_overlap\": n_pairs\n",
    "            })\n",
    "\n",
    "    result_df = pd.DataFrame.from_records(recs)\n",
    "\n",
    "    # pivot for heatmaps\n",
    "    if not result_df.empty:\n",
    "        heat_corr = result_df.pivot(\n",
    "            index=\"STL_horizon_hr\",\n",
    "            columns=\"CHS_horizon_hr\",\n",
    "            values=\"max_corr\"\n",
    "        )\n",
    "        heat_lag = result_df.pivot(\n",
    "            index=\"STL_horizon_hr\",\n",
    "            columns=\"CHS_horizon_hr\",\n",
    "            values=\"best_lag_days\"\n",
    "        )\n",
    "    else:\n",
    "        # empty fallback\n",
    "        heat_corr = pd.DataFrame()\n",
    "        heat_lag = pd.DataFrame()\n",
    "\n",
    "    return result_df, heat_corr, heat_lag\n",
    "\n",
    "\n",
    "def plot_heatmap(matrix_df, title, xlabel, ylabel, cbar_label,\n",
    "                 outfile, cmap=\"viridis\", vmin=None, vmax=None,\n",
    "                 x_is_hours=True, y_is_hours=True):\n",
    "    \"\"\"\n",
    "    matrix_df is a 2D DataFrame with numeric values.\n",
    "    We'll label ticks in days (hr/24.0).\n",
    "    \"\"\"\n",
    "\n",
    "    if matrix_df.empty:\n",
    "        print(f\"[WARN] nothing to plot for {outfile}\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "    im = ax.imshow(\n",
    "        matrix_df.values,\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "        cmap=cmap,\n",
    "        vmin=vmin,\n",
    "        vmax=vmax\n",
    "    )\n",
    "\n",
    "    # tick labels\n",
    "    x_vals = list(matrix_df.columns)\n",
    "    y_vals = list(matrix_df.index)\n",
    "\n",
    "    if x_is_hours:\n",
    "        x_ticklabels = [f\"{h/24:.1f}d\" for h in x_vals]\n",
    "    else:\n",
    "        x_ticklabels = [str(x) for x in x_vals]\n",
    "\n",
    "    if y_is_hours:\n",
    "        y_ticklabels = [f\"{h/24:.1f}d\" for h in y_vals]\n",
    "    else:\n",
    "        y_ticklabels = [str(y) for y in y_vals]\n",
    "\n",
    "    ax.set_xticks(range(len(x_vals)))\n",
    "    ax.set_xticklabels(x_ticklabels, rotation=45)\n",
    "\n",
    "    ax.set_yticks(range(len(y_vals)))\n",
    "    ax.set_yticklabels(y_ticklabels)\n",
    "\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.set_label(cbar_label)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(outfile, dpi=200)\n",
    "    plt.close(fig)\n",
    "    print(f\"Wrote {outfile}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load both stations' ensemble-mean data\n",
    "    stl_df = load_station_df(stl_file,     obs_col, time_col)\n",
    "    chs_df = load_station_df(chester_file, obs_col, time_col)\n",
    "\n",
    "    # We'll do this for dry and wet separately\n",
    "    for season_label in [\"dry\", \"wet\"]:\n",
    "        print(f\"Processing season: {season_label}\")\n",
    "\n",
    "        result_df, heat_corr, heat_lag = compute_season_matrix(\n",
    "            stl_df,\n",
    "            chs_df,\n",
    "            stl_horizons_hours,\n",
    "            chs_horizons_hours,\n",
    "            season_label,\n",
    "            max_lag_days,\n",
    "            lag_step_hours,\n",
    "            obs_col\n",
    "        )\n",
    "\n",
    "        # save numeric summary for this season\n",
    "        season_csv = out_dir / f\"lag_corr_matrix_STL_vs_CHS_{season_label}.csv\"\n",
    "        result_df.to_csv(season_csv, index=False)\n",
    "        print(f\"Wrote {season_csv}\")\n",
    "\n",
    "        # plot max correlation heatmap for this season\n",
    "        corr_png = out_dir / f\"lag_corr_heatmap_corr_{season_label}.png\"\n",
    "        plot_heatmap(\n",
    "            heat_corr,\n",
    "            title=f\"Max correlation (STL error vs CHS error), {season_label} season\",\n",
    "            xlabel=\"Chester lead time (days)\",\n",
    "            ylabel=\"St. Louis lead time (days)\",\n",
    "            cbar_label=\"corr (Pearson r)\",\n",
    "            outfile=corr_png,\n",
    "            cmap=\"plasma\",        # good contrast for correlation\n",
    "            vmin=0.0, vmax=1.0\n",
    "        )\n",
    "\n",
    "        # plot best lag heatmap for this season\n",
    "        # We'll use a diverging cmap centered on 0 days lag\n",
    "        lag_png = out_dir / f\"lag_corr_heatmap_lagdays_{season_label}.png\"\n",
    "        # Estimate symmetric range for color scale\n",
    "        # We scan heat_lag to get max abs lag to set vmin/vmax balanced\n",
    "        if not heat_lag.empty:\n",
    "            max_abs_lag = np.nanmax(np.abs(heat_lag.values))\n",
    "            if np.isnan(max_abs_lag):\n",
    "                max_abs_lag = 2.0  # fallback\n",
    "            vmax_lag = max_abs_lag\n",
    "        else:\n",
    "            vmax_lag = 2.0\n",
    "\n",
    "        plot_heatmap(\n",
    "            heat_lag,\n",
    "            title=f\"Best lag in days (positive = CHS later), {season_label} season\",\n",
    "            xlabel=\"Chester lead time (days)\",\n",
    "            ylabel=\"St. Louis lead time (days)\",\n",
    "            cbar_label=\"lag (days)\",\n",
    "            outfile=lag_png,\n",
    "            cmap=\"RdBu_r\",        # blue=negative lag, red=positive lag\n",
    "            vmin=-vmax_lag, vmax=vmax_lag\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
