{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64056eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Member 01: downloading CFSv2 PRATE as 6h accumulations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                       | 0/121 [00:00<?, ?it/s]Ignoring index file ':memory:' older than GRIB file\n",
      "  0%|                                                                       | 0/121 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions {'time'} do not exist. Expected one or more of ('lat', 'lon')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 505\u001b[39m\n\u001b[32m    503\u001b[39m ymd = sys.argv[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sys.argv) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    504\u001b[39m cyc = sys.argv[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sys.argv) > \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m run_one_cycle(ymd, cyc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 415\u001b[39m, in \u001b[36mrun_one_cycle\u001b[39m\u001b[34m(yyyymmdd, cycle)\u001b[39m\n\u001b[32m    413\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m bname, poly \u001b[38;5;129;01min\u001b[39;00m basins.items():\n\u001b[32m    414\u001b[39m         masked = mask_to_basin(sixh, poly)\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m         mean_series = area_average_mm(masked)  \u001b[38;5;66;03m# one-time step -> one value\u001b[39;00m\n\u001b[32m    416\u001b[39m         cfs_area_means[bname].append(\u001b[38;5;28mfloat\u001b[39m(mean_series.iloc[\u001b[32m0\u001b[39m]))\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid_times:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 290\u001b[39m, in \u001b[36marea_average_mm\u001b[39m\u001b[34m(da_masked)\u001b[39m\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mExpected \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlat\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlon\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    289\u001b[39m w = np.cos(np.deg2rad(arr[\u001b[33m\"\u001b[39m\u001b[33mlat\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m w2d = xr.ones_like(arr.isel(time=\u001b[32m0\u001b[39m)) * w\n\u001b[32m    291\u001b[39m w2d = w2d.where(arr.isel(time=\u001b[32m0\u001b[39m).notnull())\n\u001b[32m    292\u001b[39m num = (arr * w2d).sum(dim=(\u001b[33m\"\u001b[39m\u001b[33mlat\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mlon\u001b[39m\u001b[33m\"\u001b[39m), skipna=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/LRF_RC/lib/python3.12/site-packages/xarray/core/dataarray.py:1535\u001b[39m, in \u001b[36mDataArray.isel\u001b[39m\u001b[34m(self, indexers, drop, missing_dims, **indexers_kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._from_temp_dataset(ds)\n\u001b[32m   1532\u001b[39m \u001b[38;5;66;03m# Much faster algorithm for when all indexers are ints, slices, one-dimensional\u001b[39;00m\n\u001b[32m   1533\u001b[39m \u001b[38;5;66;03m# lists, or zero or one-dimensional np.ndarray's\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1535\u001b[39m variable = \u001b[38;5;28mself\u001b[39m._variable.isel(indexers, missing_dims=missing_dims)\n\u001b[32m   1536\u001b[39m indexes, index_variables = isel_indexes(\u001b[38;5;28mself\u001b[39m.xindexes, indexers)\n\u001b[32m   1538\u001b[39m coords = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/LRF_RC/lib/python3.12/site-packages/xarray/core/variable.py:1035\u001b[39m, in \u001b[36mVariable.isel\u001b[39m\u001b[34m(self, indexers, missing_dims, **indexers_kwargs)\u001b[39m\n\u001b[32m   1011\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a new array indexed along the specified dimension(s).\u001b[39;00m\n\u001b[32m   1012\u001b[39m \n\u001b[32m   1013\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1031\u001b[39m \u001b[33;03m    indexer, in which case the data will be a copy.\u001b[39;00m\n\u001b[32m   1032\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1033\u001b[39m indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \u001b[33m\"\u001b[39m\u001b[33misel\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1035\u001b[39m indexers = drop_dims_from_indexers(indexers, \u001b[38;5;28mself\u001b[39m.dims, missing_dims)\n\u001b[32m   1037\u001b[39m key = \u001b[38;5;28mtuple\u001b[39m(indexers.get(dim, \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dims)\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/LRF_RC/lib/python3.12/site-packages/xarray/core/utils.py:797\u001b[39m, in \u001b[36mdrop_dims_from_indexers\u001b[39m\u001b[34m(indexers, dims, missing_dims)\u001b[39m\n\u001b[32m    795\u001b[39m     invalid = indexers.keys() - \u001b[38;5;28mset\u001b[39m(dims)\n\u001b[32m    796\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m invalid:\n\u001b[32m--> \u001b[39m\u001b[32m797\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    798\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDimensions \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m do not exist. Expected one or more of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdims\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    799\u001b[39m         )\n\u001b[32m    801\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m indexers\n\u001b[32m    803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m missing_dims == \u001b[33m\"\u001b[39m\u001b[33mwarn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    804\u001b[39m     \u001b[38;5;66;03m# don't modify input\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Dimensions {'time'} do not exist. Expected one or more of ('lat', 'lon')"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Real-time evaluation of CFSv2 precipitation initialization error\n",
    "over HUC2=07 (Upper Mississippi) and HUC2=10 (Missouri) basins.\n",
    "\n",
    "Requirements:\n",
    "  python -m pip install xarray cfgrib==0.9.14.1 eccodes requests pandas numpy geopandas shapely pyproj rioxarray tqdm pytz\n",
    "\n",
    "Notes:\n",
    "- CFSv2 operational archive is a 7-day rolling window. Run this daily or more often.\n",
    "- We read CFSv2 6-hr \"flux\" (flxf*.grb2) files, variable shortName='prate'.\n",
    "- We build 6-hr accumulations from the rate and align with MRMS hourly QPE aggregated to 6-hr windows.\n",
    "- Initialization error is reported by lead bin and by cycle/member.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import ftplib\n",
    "import pytz\n",
    "import queue\n",
    "import shutil\n",
    "import zipfile\n",
    "import logging\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray  # for raster masking\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "from shapely.geometry import shape, mapping\n",
    "from shapely.ops import unary_union\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def _load_cfgrib(grib_bytes: bytes, fkeys: dict | None = None) -> xr.Dataset:\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".grib2\", delete=False) as tf:\n",
    "        tf.write(grib_bytes)\n",
    "        tf.flush()\n",
    "        tmp_path = tf.name\n",
    "    try:\n",
    "        backend_kwargs = {\"indexpath\": \":memory:\"}\n",
    "        if fkeys:\n",
    "            backend_kwargs[\"filter_by_keys\"] = fkeys\n",
    "        with xr.open_dataset(tmp_path, engine=\"cfgrib\", backend_kwargs=backend_kwargs) as ds:\n",
    "            ds = ds.load()\n",
    "        if not ds.data_vars:\n",
    "            raise RuntimeError(f\"cfgrib open produced empty dataset for filter={fkeys}\")\n",
    "        return ds\n",
    "    finally:\n",
    "        try: os.remove(tmp_path)\n",
    "        except Exception: pass\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "NOMADS_BASE = \"https://nomads.ncep.noaa.gov/pub/data/nccf/com/cfs/prod\"\n",
    "# We will pull \"flxf\" 6-hourly flux files from: {NOMADS_BASE}/cfs.YYYYMMDD/CC/6hrly_grib_0M/\n",
    "# where M = 1..4 is ensemble member, CC in {00,06,12,18}\n",
    "\n",
    "MRMS_BASE = \"https://mrms.ncep.noaa.gov/2D/MultiSensor_QPE_01H_Pass2\"\n",
    "\n",
    "# HUC2 polygons via USGS WBD MapServer layer 1 (HUC2)\n",
    "WBD_HUC2_ENDPOINT = \"https://hydro.nationalmap.gov/arcgis/rest/services/wbd/MapServer/1/query\"\n",
    "\n",
    "# Spatial resampling method for basin averaging\n",
    "CFS_RESAMPLING = \"nearest\"    # options: \"nearest\", \"bilinear\" (xarray's \"linear\" via rioxarray)\n",
    "MRMS_RESAMPLING = \"nearest\"\n",
    "\n",
    "# Timezone for outputs\n",
    "TZ = \"America/Chicago\"\n",
    "\n",
    "# Max forecast days to evaluate (NWM LRF uses first 30 days)\n",
    "MAX_FCST_DAYS = 30\n",
    "\n",
    "# Batches\n",
    "DOWNLOAD_DIR = Path(\"./data_rt\")\n",
    "DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------\n",
    "# Utility helpers\n",
    "# -----------------------\n",
    "\n",
    "def latest_cfs_cycle() -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Discover the latest available CFSv2 cycle and its date folder by scraping the index.\n",
    "    Returns (yyyyMMdd, cycle='00'|'06'|'12'|'18').\n",
    "    \"\"\"\n",
    "    idx = requests.get(NOMADS_BASE + \"/\").text\n",
    "    # find cfs.YYYYMMDD/ entries\n",
    "    dates = sorted(re.findall(r'href=\"cfs\\.(\\d{8})/', idx))\n",
    "    if not dates:\n",
    "        raise RuntimeError(\"No cfs.YYYYMMDD directories found on NOMADS.\")\n",
    "    last_day = dates[-1]\n",
    "    day_idx = requests.get(f\"{NOMADS_BASE}/cfs.{last_day}/\").text\n",
    "    cycles = sorted(re.findall(r'href=\"(00|06|12|18)/\"', day_idx))\n",
    "    if not cycles:\n",
    "        raise RuntimeError(f\"No cycles found for cfs.{last_day}\")\n",
    "    # Use the latest present cycle\n",
    "    cyc = cycles[-1]\n",
    "    return last_day, cyc\n",
    "\n",
    "def list_cfs_member_dir(yyyymmdd: str, cycle: str, member: int) -> str:\n",
    "    # member folders are 6hrly_grib_01..04\n",
    "    return f\"{NOMADS_BASE}/cfs.{yyyymmdd}/{cycle}/6hrly_grib_{member:02d}/\"\n",
    "\n",
    "def get_huc2_polygon(huc2: str) -> gpd.GeoSeries:\n",
    "    \"\"\"\n",
    "    Fetch HUC2 polygon from USGS WBD service as GeoJSON.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"where\": f\"HUC2='{huc2}'\",\n",
    "        \"outFields\": \"*\",\n",
    "        \"f\": \"geojson\",\n",
    "        \"outSR\": 4326\n",
    "    }\n",
    "    r = requests.get(WBD_HUC2_ENDPOINT, params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    gj = r.json()\n",
    "    feats = [shape(f[\"geometry\"]) for f in gj.get(\"features\", []) if f.get(\"geometry\")]\n",
    "    if not feats:\n",
    "        raise RuntimeError(f\"No polygon returned for HUC2={huc2}\")\n",
    "    union = unary_union(feats)\n",
    "    return gpd.GeoSeries([union], crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def open_cfs_precip_6h(grib_bytes: bytes) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Open a CFS flxf GRIB and return 6-hour precipitation accumulation as a DataArray named 'cfs_prcp_6h' in mm.\n",
    "    Handles either:\n",
    "      - PRATE (rate), stepType=avg  -> multiply by 6*3600\n",
    "      - APCP/TP (accumulation), stepType=accum -> convert units if needed\n",
    "    \"\"\"\n",
    "    if not (len(grib_bytes) >= 4 and grib_bytes[:4] == b\"GRIB\"):\n",
    "        raise ValueError(\"Bytes are not a GRIB file (missing GRIB header).\")\n",
    "\n",
    "    # Try PRATE avg at surface first (most common in flxf)\n",
    "    try:\n",
    "        ds = _load_cfgrib(grib_bytes, {\n",
    "            \"shortName\": \"prate\",\n",
    "            \"typeOfLevel\": \"surface\",\n",
    "            \"stepType\": \"avg\"\n",
    "        })\n",
    "        da = ds[\"prate\"]\n",
    "        sixh = da * (6 * 3600.0)  # kg m-2 s-1 -> mm per 6h\n",
    "        sixh = sixh.rename(\"cfs_prcp_6h\")\n",
    "        sixh.attrs[\"units\"] = \"mm/6h\"\n",
    "        return sixh\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Try PRATE without strict keys\n",
    "    try:\n",
    "        ds = _load_cfgrib(grib_bytes, {\"shortName\": \"prate\"})\n",
    "        da = ds[\"prate\"]\n",
    "        # If stepType is avg or mean over 6h, convert to mm/6h.\n",
    "        # Some files encode exact averaging interval via 'step' coordinate.\n",
    "        factor = 6 * 3600.0\n",
    "        if \"step\" in da.coords:\n",
    "            # Use exact step seconds if present\n",
    "            try:\n",
    "                step_sec = pd.to_timedelta(da[\"step\"].values).astype(\"timedelta64[s]\").astype(int)\n",
    "                # Many files have a single step; if array, take first\n",
    "                if np.ndim(step_sec) > 0:\n",
    "                    step_sec = int(np.array(step_sec).ravel()[0])\n",
    "                factor = float(step_sec)\n",
    "            except Exception:\n",
    "                pass\n",
    "        sixh = da * factor\n",
    "        sixh = sixh.rename(\"cfs_prcp_6h\")\n",
    "        sixh.attrs[\"units\"] = \"mm/6h\"\n",
    "        return sixh\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Try accumulated precip variants (APCP). Depending on tables, shortName may be 'apcp' or 'tp'.\n",
    "    for sn in (\"apcp\", \"tp\"):\n",
    "        try:\n",
    "            # accumulated over period (stepType=accum), surface\n",
    "            ds = _load_cfgrib(grib_bytes, {\n",
    "                \"shortName\": sn,\n",
    "                \"typeOfLevel\": \"surface\",\n",
    "                \"stepType\": \"accum\"\n",
    "            })\n",
    "        except Exception:\n",
    "            # try with only shortName\n",
    "            try:\n",
    "                ds = _load_cfgrib(grib_bytes, {\"shortName\": sn})\n",
    "            except Exception:\n",
    "                ds = None\n",
    "        if ds is not None and len(ds.data_vars) > 0:\n",
    "            varname = list(ds.data_vars)[0] if sn not in ds.data_vars else sn\n",
    "            da = ds[varname]\n",
    "            # Units can be 'm' for ECMWF-like 'tp'; convert to mm\n",
    "            units = da.attrs.get(\"units\", \"\").lower()\n",
    "            if units in (\"m\", \"meter\", \"metre\", \"meters\", \"metres\"):\n",
    "                da_mm = da * 1000.0\n",
    "            else:\n",
    "                # Many NCEP APCP fields arrive already as mm\n",
    "                da_mm = da\n",
    "            sixh = da_mm.rename(\"cfs_prcp_6h\")\n",
    "            sixh.attrs[\"units\"] = \"mm/6h\"\n",
    "            return sixh\n",
    "\n",
    "    # Final fall-back: open without filters so we can report what's inside\n",
    "    try:\n",
    "        anyds = _load_cfgrib(grib_bytes, None)\n",
    "        vars_found = list(anyds.data_vars)\n",
    "        raise RuntimeError(f\"Could not locate precip in GRIB. Variables present: {vars_found}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"Could not open GRIB with cfgrib at all. Verify eccodes and cfgrib installations.\"\n",
    "        ) from e\n",
    "\n",
    "def fetch_url(url: str, retry=5, sleep=5) -> bytes:\n",
    "    for k in range(retry):\n",
    "        r = requests.get(url, timeout=120)\n",
    "        if r.ok:\n",
    "            b = r.content\n",
    "            # Accept GRIB2 or gzip depending on URL suffix\n",
    "            if url.endswith(\".grb2\") or url.endswith(\".grib2\"):\n",
    "                if len(b) >= 4 and b[:4] == b\"GRIB\":\n",
    "                    return b\n",
    "            elif url.endswith(\".gz\"):\n",
    "                if len(b) >= 2 and b[:2] == b\"\\x1f\\x8b\":\n",
    "                    return b\n",
    "            else:\n",
    "                # If some other suffix shows up, just return on 200 OK\n",
    "                return b\n",
    "        time.sleep(sleep * (1.5 ** k))\n",
    "    raise RuntimeError(f\"Failed to fetch expected content from {url}\")\n",
    "\n",
    "\n",
    "\n",
    "def expected_flxf_filenames(init_yyyymmdd: str, init_cycle: str, member: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    CFSv2 6-hr flux filenames follow:\n",
    "    flxfYYYYmmddHH.mm.VALIDyyyymmddHH.grb2\n",
    "      where 'mm' is ensemble member 01..04, rightmost timestamp is init,\n",
    "      left timestamp is 'valid time' every 6 hours.\n",
    "    \"\"\"\n",
    "    init = dt.datetime.strptime(init_yyyymmdd + init_cycle, \"%Y%m%d%H\").replace(tzinfo=dt.timezone.utc)\n",
    "    names = []\n",
    "    # 6-hour steps through first 30 days (inclusive end is OK; NOMADS may lag)\n",
    "    steps = int((MAX_FCST_DAYS * 24) / 6)\n",
    "    for s in range(0, steps + 1):\n",
    "        valid = init + dt.timedelta(hours=6 * s)\n",
    "        left = valid.strftime(\"%Y%m%d%H\")\n",
    "        right = init.strftime(\"%Y%m%d%H\")\n",
    "        names.append(f\"flxf{left}.{member:02d}.{right}.grb2\")\n",
    "    return names\n",
    "\n",
    "\n",
    "def mask_to_basin(da: xr.DataArray, basin: gpd.GeoSeries) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Reproject basin to dataset CRS and mask the raster.\n",
    "    \"\"\"\n",
    "    if \"latitude\" in da.coords and \"longitude\" in da.coords:\n",
    "        da = da.rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n",
    "    # ensure CRS\n",
    "    da = da.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "    # vector to same CRS\n",
    "    basin = basin.to_crs(\"EPSG:4326\")\n",
    "    masked = da.rio.clip(basin.geometry.apply(mapping), basin.crs, drop=False)\n",
    "    return masked\n",
    "\n",
    "def hour_end_from_mrms_name(url: str) -> pd.Timestamp:\n",
    "    m = re.search(r\"_(\\d{8})-(\\d{2})0000\", url)\n",
    "    if not m:\n",
    "        raise RuntimeError(f\"Cannot parse MRMS hour from {url}\")\n",
    "    return pd.to_datetime(m.group(1) + m.group(2), format=\"%Y%m%d%H\", utc=True)\n",
    "\n",
    "def area_average_mm(da_masked: xr.DataArray) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Cosine-lat weighted basin mean.\n",
    "    Returns:\n",
    "      - a pandas Series indexed by time if 'time' dim exists\n",
    "      - a length-1 Series if there is no 'time' dim\n",
    "    \"\"\"\n",
    "    arr = da_masked.where(np.isfinite(da_masked))\n",
    "\n",
    "    if \"lat\" not in arr.coords or \"lon\" not in arr.coords:\n",
    "        raise ValueError(\"Expected 'lat' and 'lon' coordinates.\")\n",
    "\n",
    "    # weights ~ cos(lat), broadcast over lon (and time, if present)\n",
    "    w_lat = np.cos(np.deg2rad(arr[\"lat\"]))\n",
    "    w = w_lat.broadcast_like(arr)\n",
    "\n",
    "    # weighted spatial mean\n",
    "    num = (arr * w).sum(dim=(\"lat\", \"lon\"), skipna=True)\n",
    "    den = w.sum(dim=(\"lat\", \"lon\"), skipna=True)\n",
    "    mean_da = num / den\n",
    "\n",
    "    if \"time\" in mean_da.dims:\n",
    "        s = mean_da.to_series()\n",
    "        s.name = arr.name\n",
    "        return s\n",
    "    else:\n",
    "        # single timestamp file -> scalar; return length-1 Series to match callers\n",
    "        val = float(mean_da.values)\n",
    "        s = pd.Series([val], name=arr.name)\n",
    "        return s\n",
    "\n",
    "\n",
    "\n",
    "def list_mrms_hourlies(start_utc: pd.Timestamp, end_utc: pd.Timestamp) -> List[str]:\n",
    "    \"\"\"\n",
    "    Build list of MRMS hourly files covering [start, end].\n",
    "    File pattern: MRMS_MultiSensor_QPE_01H_Pass2_00.00_YYYYMMDD-HH0000.grib2.gz\n",
    "    \"\"\"\n",
    "    hours = pd.date_range(start_utc.floor(\"H\"), end_utc.ceil(\"H\"), freq=\"H\", tz=\"UTC\")\n",
    "    return [\n",
    "        f\"{MRMS_BASE}/MRMS_MultiSensor_QPE_01H_Pass2_00.00_{t.strftime('%Y%m%d-%H0000')}.grib2.gz\"\n",
    "        for t in hours\n",
    "    ]\n",
    "\n",
    "def open_mrms_hourly_mm_gz(gz_bytes: bytes) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Open MRMS hourly QPE GRIB2.gz as mm/hour accumulation over previous hour.\n",
    "    \"\"\"\n",
    "    with gzip.GzipFile(fileobj=io.BytesIO(gz_bytes)) as gz:\n",
    "        with xr.open_dataset(gz, engine=\"cfgrib\") as ds:\n",
    "            # MRMS uses 'unknown' variable names sometimes; try typical names\n",
    "            # Many MRMS GRIBs carry 'unknown' shortName with discipline=Hydrology.\n",
    "            # Pull the first data variable.\n",
    "            vname = [k for k in ds.data_vars][0]\n",
    "            da = ds[vname].rename(\"mrms_qpe_1h\")\n",
    "            da.attrs[\"units\"] = \"mm\"\n",
    "            da = da.rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n",
    "            return da.load()\n",
    "\n",
    "def aggregate_mrms_to_6h(mm_1h_series: Dict[pd.Timestamp, float]) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Sum hourly MRMS to 6-h windows ending at 00,06,12,18 UTC to align with CFS 6h valid times.\n",
    "    \"\"\"\n",
    "    s = pd.Series(mm_1h_series, dtype=float)\n",
    "    s.index = pd.to_datetime(s.index, utc=True)\n",
    "    # Define windows with labels at window end\n",
    "    s6 = s.resample(\"6H\", label=\"right\", closed=\"right\").sum(min_count=1)\n",
    "    s6.name = \"mrms_prcp_6h\"\n",
    "    return s6\n",
    "\n",
    "def evaluate_init_error(cfs_6h: pd.Series, mrms_6h: pd.Series, init_utc: pd.Timestamp) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge series and compute error metrics by lead hours.\n",
    "    \"\"\"\n",
    "    df = pd.concat([cfs_6h.rename(\"cfs\"), mrms_6h.rename(\"obs\")], axis=1).dropna()\n",
    "    df[\"lead_hours\"] = (df.index.tz_convert(\"UTC\") - init_utc).total_seconds() / 3600.0\n",
    "    df = df[(df[\"lead_hours\"] >= 0) & (df[\"lead_hours\"] <= MAX_FCST_DAYS * 24)]\n",
    "    df[\"err\"] = df[\"cfs\"] - df[\"obs\"]\n",
    "    return df\n",
    "\n",
    "def bin_metrics(df: pd.DataFrame, bins_hours=(0, 6, 12, 24, 48, 72, 120, 240, 360, 720)) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute mean error (bias), MAE, RMSE by lead bins.\n",
    "    \"\"\"\n",
    "    cats = pd.cut(df[\"lead_hours\"], bins=bins_hours, right=True, include_lowest=True)\n",
    "    out = df.groupby(cats).agg(\n",
    "        n=(\"err\", \"count\"),\n",
    "        bias_mm=(\"err\", \"mean\"),\n",
    "        mae_mm=(\"err\", lambda x: np.mean(np.abs(x))),\n",
    "        rmse_mm=(\"err\", lambda x: math.sqrt(np.mean(x**2))),\n",
    "        obs_mm=(\"obs\", \"mean\"),\n",
    "        fcst_mm=(\"cfs\", \"mean\")\n",
    "    ).reset_index().rename(columns={\"lead_hours\": \"lead_bin\"})\n",
    "    return out\n",
    "\n",
    "# -----------------------\n",
    "# Main driver\n",
    "# -----------------------\n",
    "MAX_MISS = 8\n",
    "\n",
    "def valid_time_from_name(fn: str) -> pd.Timestamp:\n",
    "    m = re.search(r\"flxf(\\d{10})\\.\\d{2}\\.(\\d{10})\\.grb2$\", fn)\n",
    "    if not m:\n",
    "        raise RuntimeError(f\"Cannot parse valid time from {fn}\")\n",
    "    return pd.to_datetime(m.group(1), format=\"%Y%m%d%H\", utc=True)\n",
    "\n",
    "\n",
    "def run_one_cycle(yyyymmdd: str = None, cycle: str = None):\n",
    "    if yyyymmdd is None or cycle is None:\n",
    "        yyyymmdd, cycle = latest_cfs_cycle()\n",
    "\n",
    "    init_utc = pd.to_datetime(f\"{yyyymmdd}{cycle}\", format=\"%Y%m%d%H\", utc=True)\n",
    "\n",
    "    # Get basins\n",
    "    basins = {\n",
    "        \"HUC2_07_UpperMiss\": get_huc2_polygon(\"07\"),\n",
    "        \"HUC2_10_Missouri\": get_huc2_polygon(\"10\"),\n",
    "    }\n",
    "\n",
    "    all_metrics = []\n",
    "\n",
    "    for member in [1, 2, 3, 4]:\n",
    "        member_dir = list_cfs_member_dir(yyyymmdd, cycle, member)\n",
    "        fnames = expected_flxf_filenames(yyyymmdd, cycle, member)\n",
    "\n",
    "        # We'll only iterate until files stop existing\n",
    "        # Build time span to retrieve MRMS later\n",
    "        valid_times = []\n",
    "        cfs_area_means = {k: [] for k in basins.keys()}\n",
    "\n",
    "        print(f\"\\nMember {member:02d}: downloading CFSv2 PRATE as 6h accumulations...\")\n",
    "        miss_streak = 0\n",
    "        for fn in tqdm(fnames, ncols=100):\n",
    "            url = member_dir + fn\n",
    "            try:\n",
    "                grib = fetch_url(url)\n",
    "                miss_streak = 0\n",
    "            except Exception:\n",
    "                miss_streak += 1\n",
    "                if miss_streak >= MAX_MISS:\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            sixh = open_cfs_precip_6h(grib)  # mm per 6h\n",
    "            t_valid = valid_time_from_name(fn)  # use filename as single source of truth\n",
    "            valid_times.append(t_valid)\n",
    "\n",
    "            for bname, poly in basins.items():\n",
    "                masked = mask_to_basin(sixh, poly)\n",
    "                mean_series = area_average_mm(masked)  # one-time step -> one value\n",
    "                cfs_area_means[bname].append(float(mean_series.iloc[0]))\n",
    "\n",
    "        if not valid_times:\n",
    "            print(f\"Member {member:02d}: no files found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Build series per basin\n",
    "        for bname in basins.keys():\n",
    "            # Ensure same length\n",
    "            n = min(len(cfs_area_means[bname]), len(valid_times))\n",
    "            if n == 0:\n",
    "                continue\n",
    "            times_n = pd.DatetimeIndex(valid_times[:n], tz=\"UTC\")\n",
    "            vals_n = cfs_area_means[bname][:n]\n",
    "            cfs_ser = pd.Series(vals_n, index=times_n, name=f\"cfs_{bname}_mm6h\")\n",
    "\n",
    "            # MRMS: gather hourlies spanning the same window then aggregate to 6h\n",
    "            mrms_urls = list_mrms_hourlies(cfs_ser.index.min() - pd.Timedelta(hours=6),\n",
    "                                           cfs_ser.index.max())\n",
    "            mrms_hourly_vals = {}\n",
    "            print(f\"Member {member:02d} {bname}: downloading MRMS hourlies ({len(mrms_urls)} files)...\")\n",
    "                \n",
    "            # Basin mask and spatial mean\n",
    "            masked = mask_to_basin(da, basins[bname])\n",
    "            mean_hour = float(masked.mean(dim=(\"lat\", \"lon\"), skipna=True).values)\n",
    "            # Time from GRIB 'time' coordinate\n",
    "            tcoord = pd.to_datetime(da[\"time\"].values).tz_localize(\"UTC\") if \"time\" in da.coords else None\n",
    "            if tcoord is None:\n",
    "                # Parse from filename\n",
    "                # ..._YYYYMMDD-HH0000.grib2.gz is end time of the 1-hour accumulation\n",
    "                ts = re.search(r\"_(\\d{8})-(\\d{2})0000\", u)\n",
    "                if ts:\n",
    "                    tcoord = pd.Timestamp(ts.group(1)+\"T\"+ts.group(2)+\":00:00Z\")\n",
    "                else:\n",
    "                    continue\n",
    "            mrms_hourly_vals[tcoord] = mean_hour\n",
    "\n",
    "            mrms6 = aggregate_mrms_to_6h(mrms_hourly_vals)\n",
    "\n",
    "            # align to CFS valid times\n",
    "            mrms6 = mrms6.reindex(cfs_ser.index, method=None)\n",
    "\n",
    "            # Metrics for this member and basin\n",
    "            df_err = evaluate_init_error(cfs_ser, mrms6, init_utc)\n",
    "            if df_err.empty:\n",
    "                continue\n",
    "            mtab = bin_metrics(df_err)\n",
    "            mtab[\"member\"] = member\n",
    "            mtab[\"basin\"] = bname\n",
    "            mtab[\"init_utc\"] = init_utc\n",
    "            all_metrics.append(mtab)\n",
    "\n",
    "    if not all_metrics:\n",
    "        raise SystemExit(\"No metrics computed. Likely the cycle has not fully populated yet.\")\n",
    "\n",
    "    metrics = pd.concat(all_metrics, ignore_index=True)\n",
    "    # Tidy lead bin labels\n",
    "    metrics[\"lead_bin_h\"] = metrics[\"lead_bin\"].astype(str).str.replace(r\"\\(|\\]\",\"\",regex=True)\n",
    "    # Save artifacts\n",
    "    out_dir = DOWNLOAD_DIR / f\"cfs_{yyyymmdd}{cycle}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    metrics.to_csv(out_dir / \"init_error_metrics_by_bin.csv\", index=False)\n",
    "\n",
    "    # Also produce a quick wide summary: mean over members by basin and lead bin\n",
    "    summary = (metrics\n",
    "               .groupby([\"basin\",\"lead_bin_h\"], as_index=False)\n",
    "               .agg(bias_mm=(\"bias_mm\",\"mean\"),\n",
    "                    mae_mm=(\"mae_mm\",\"mean\"),\n",
    "                    rmse_mm=(\"rmse_mm\",\"mean\"),\n",
    "                    n=(\"n\",\"sum\")))\n",
    "    summary.to_csv(out_dir / \"init_error_summary_mean_over_members.csv\", index=False)\n",
    "    print(f\"\\nWrote:\\n  {out_dir/'init_error_metrics_by_bin.csv'}\\n  {out_dir/'init_error_summary_mean_over_members.csv'}\")\n",
    "    print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Optional CLI args: YYYYMMDD CC\n",
    "    ymd = sys.argv[1] if len(sys.argv) > 1 else None\n",
    "    cyc = sys.argv[2] if len(sys.argv) > 2 else None\n",
    "    run_one_cycle(ymd, cyc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b267f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eccodes: 2.44.0\n",
      "cfgrib: 0.9.14.1\n",
      "xarray: 2025.8.0\n"
     ]
    }
   ],
   "source": [
    "import eccodes, cfgrib, xarray as xr\n",
    "print(\"eccodes:\", getattr(eccodes, '__version__', 'unknown'))\n",
    "print(\"cfgrib:\", getattr(cfgrib, '__version__', 'unknown'))\n",
    "print(\"xarray:\", xr.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LRF_RC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
