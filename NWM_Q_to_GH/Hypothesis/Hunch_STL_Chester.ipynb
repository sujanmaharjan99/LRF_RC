{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c44f9dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote summary table: /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/Hypothesis/lag_corr_matrix_STL_vs_CHS.csv\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/Hypothesis/lag_corr_heatmap_corr.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/Hypothesis/lag_corr_heatmap_lagdays.png\n",
      "Wrote /media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/Hypothesis/lag_corr_scatter_strong.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "########################################\n",
    "# User config\n",
    "########################################\n",
    "\n",
    "out_dir = Path(\"/media/12TB/Sujan/NWM/Codes/LRF_RC/NWM_Q_to_GH/Hypothesis\")\n",
    "\n",
    "stl_file     = out_dir / \"../09_ST_Louis_All_q_tolerance_25.0_day_bracket_1000.csv\"\n",
    "chester_file = out_dir / \"../10_Chester_All_q_tolerance_25.0_day_bracket_1000.csv\"\n",
    "\n",
    "obs_col = \"stage_m_USGS_nearest\"\n",
    "time_col = \"time\"\n",
    "\n",
    "# Horizons of interest (hours as strings with zero padding to 3 digits)\n",
    "# Around 2 days for St. Louis\n",
    "stl_horizons_hours = [6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72]\n",
    "\n",
    "# Around 5 days for Chester\n",
    "chs_horizons_hours = [36, 42, 48, 54, 60, 66,72, 78, 84, 90, 96, 102, 108, 114, 120]\n",
    "\n",
    "# lag search settings\n",
    "max_lag_days = 5\n",
    "lag_step_hours = 6\n",
    "########################################\n",
    "\n",
    "\n",
    "def load_error_series(csv_path, horizon_col, obs_col, time_col):\n",
    "    \"\"\"\n",
    "    Return forecast error (pred - obs) as a pandas Series indexed by timestamp.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, parse_dates=[time_col]).set_index(time_col)\n",
    "    if horizon_col not in df.columns:\n",
    "        raise KeyError(f\"{horizon_col} not found in {csv_path.name}\")\n",
    "    if obs_col not in df.columns:\n",
    "        raise KeyError(f\"{obs_col} not found in {csv_path.name}\")\n",
    "    err = df[horizon_col] - df[obs_col]\n",
    "    err = err.dropna().sort_index()\n",
    "    return err\n",
    "\n",
    "\n",
    "def pearson_r(x, y):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    if x.size < 2 or y.size < 2:\n",
    "        return np.nan\n",
    "    r = np.corrcoef(x, y)[0, 1]\n",
    "    return float(r)\n",
    "\n",
    "\n",
    "def lagged_corr(upstream_series,\n",
    "                downstream_series,\n",
    "                max_lag_days=5,\n",
    "                lag_step_hours=6):\n",
    "    \"\"\"\n",
    "    For a pair of error series (same units), sweep lags by shifting downstream_series\n",
    "    forward/backward in time. Positive lag means downstream is moved forward\n",
    "    (arrives later). Returns a DataFrame of lag_hours, corr, N.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    max_lag_hours = max_lag_days * 24\n",
    "    lag_hours_list = np.arange(-max_lag_hours,\n",
    "                               max_lag_hours + 0.1,\n",
    "                               lag_step_hours)\n",
    "\n",
    "    for lag_h in lag_hours_list:\n",
    "        shifted = downstream_series.copy()\n",
    "        shifted.index = shifted.index + pd.Timedelta(hours=lag_h)\n",
    "\n",
    "        merged = pd.concat(\n",
    "            [upstream_series.rename(\"up\"),\n",
    "             shifted.rename(\"down\")],\n",
    "            axis=1\n",
    "        ).dropna()\n",
    "\n",
    "        if merged.empty:\n",
    "            r = np.nan\n",
    "            n = 0\n",
    "        else:\n",
    "            r = pearson_r(merged[\"up\"].values, merged[\"down\"].values)\n",
    "            n = len(merged)\n",
    "\n",
    "        results.append({\"lag_hours\": lag_h, \"corr\": r, \"N\": n})\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def best_lag_info(lag_df):\n",
    "    \"\"\"Given the output of lagged_corr, return best lag row.\"\"\"\n",
    "    lag_df_valid = lag_df.dropna(subset=[\"corr\"])\n",
    "    if lag_df_valid.empty:\n",
    "        return np.nan, np.nan, 0\n",
    "    idx_best = lag_df_valid[\"corr\"].idxmax()\n",
    "    row = lag_df_valid.loc[idx_best]\n",
    "    return float(row[\"lag_hours\"]), float(row[\"corr\"]), int(row[\"N\"])\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Preload the two station dataframes (to avoid re-reading file every loop)\n",
    "    stl_df = pd.read_csv(stl_file, parse_dates=[time_col]).set_index(time_col).sort_index()\n",
    "    chs_df = pd.read_csv(chester_file, parse_dates=[time_col]).set_index(time_col).sort_index()\n",
    "\n",
    "    # We'll store results for every combination\n",
    "    records = []\n",
    "\n",
    "    for h_up in stl_horizons_hours:\n",
    "        col_up = f\"{h_up:03d}\"\n",
    "        if col_up not in stl_df.columns:\n",
    "            print(f\"[WARN] {col_up} not found in {stl_file.name}, skipping\")\n",
    "            continue\n",
    "        stl_err = (stl_df[col_up] - stl_df[obs_col]).dropna().sort_index()\n",
    "\n",
    "        for h_down in chs_horizons_hours:\n",
    "            col_down = f\"{h_down:03d}\"\n",
    "            if col_down not in chs_df.columns:\n",
    "                print(f\"[WARN] {col_down} not found in {chester_file.name}, skipping\")\n",
    "                continue\n",
    "            chs_err = (chs_df[col_down] - chs_df[obs_col]).dropna().sort_index()\n",
    "\n",
    "            # compute lag correlation curve\n",
    "            lag_df = lagged_corr(\n",
    "                upstream_series=stl_err,\n",
    "                downstream_series=chs_err,\n",
    "                max_lag_days=max_lag_days,\n",
    "                lag_step_hours=lag_step_hours\n",
    "            )\n",
    "\n",
    "            # get best lag\n",
    "            lag_hours, max_corr, n_pairs = best_lag_info(lag_df)\n",
    "\n",
    "            records.append({\n",
    "                \"STL_horizon_hr\": h_up,\n",
    "                \"CHS_horizon_hr\": h_down,\n",
    "                \"best_lag_hours\": lag_hours,\n",
    "                \"best_lag_days\": lag_hours/24.0 if not np.isnan(lag_hours) else np.nan,\n",
    "                \"max_corr\": max_corr,\n",
    "                \"N_overlap\": n_pairs\n",
    "            })\n",
    "\n",
    "    result_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "    # save numeric summary\n",
    "    matrix_csv = out_dir / \"lag_corr_matrix_STL_vs_CHS.csv\"\n",
    "    result_df.to_csv(matrix_csv, index=False)\n",
    "    print(f\"Wrote summary table: {matrix_csv}\")\n",
    "\n",
    "    # pivot for heatmap of max_corr\n",
    "    heat_corr = result_df.pivot(\n",
    "        index=\"STL_horizon_hr\",\n",
    "        columns=\"CHS_horizon_hr\",\n",
    "        values=\"max_corr\"\n",
    "    )\n",
    "\n",
    "    # pivot for best lag (days)\n",
    "    heat_lag = result_df.pivot(\n",
    "        index=\"STL_horizon_hr\",\n",
    "        columns=\"CHS_horizon_hr\",\n",
    "        values=\"best_lag_days\"\n",
    "    )\n",
    "\n",
    "    # quick heatmap of correlation strength\n",
    "    fig1, ax1 = plt.subplots(figsize=(8,6))\n",
    "    im1 = ax1.imshow(heat_corr, aspect=\"auto\", origin=\"lower\",cmap=\"coolwarm\")\n",
    "    ax1.set_xticks(range(len(heat_corr.columns)))\n",
    "    ax1.set_xticklabels([f\"{c/24:.1f}d\" for c in heat_corr.columns], rotation=45)\n",
    "    ax1.set_yticks(range(len(heat_corr.index)))\n",
    "    ax1.set_yticklabels([f\"{r/24:.1f}d\" for r in heat_corr.index])\n",
    "    ax1.set_xlabel(\"Chester lead time (days)\")\n",
    "    ax1.set_ylabel(\"St. Louis lead time (days)\")\n",
    "    ax1.set_title(\"Max correlation (STL error vs CHS error)\")\n",
    "    cbar1 = fig1.colorbar(im1, ax=ax1)\n",
    "    cbar1.set_label(\"corr (Pearson r)\")\n",
    "    fig1.tight_layout()\n",
    "    heatmap_corr_png = out_dir / \"lag_corr_heatmap_corr.png\"\n",
    "    fig1.savefig(heatmap_corr_png, dpi=200)\n",
    "    plt.close(fig1)\n",
    "    print(f\"Wrote {heatmap_corr_png}\")\n",
    "\n",
    "    # heatmap of best lag in days (travel time)\n",
    "    fig2, ax2 = plt.subplots(figsize=(8,6))\n",
    "    im2 = ax2.imshow(heat_lag, aspect=\"auto\", origin=\"lower\")\n",
    "    ax2.set_xticks(range(len(heat_lag.columns)))\n",
    "    ax2.set_xticklabels([f\"{c/24:.1f}d\" for c in heat_lag.columns], rotation=45)\n",
    "    ax2.set_yticks(range(len(heat_lag.index)))\n",
    "    ax2.set_yticklabels([f\"{r/24:.1f}d\" for r in heat_lag.index])\n",
    "    ax2.set_xlabel(\"Chester lead time (days)\")\n",
    "    ax2.set_ylabel(\"St. Louis lead time (days)\")\n",
    "    ax2.set_title(\"Best lag in days (positive = CHS later)\")\n",
    "    cbar2 = fig2.colorbar(im2, ax2)\n",
    "    cbar2.set_label(\"lag (days)\")\n",
    "    fig2.tight_layout()\n",
    "    heatmap_lag_png = out_dir / \"lag_corr_heatmap_lagdays.png\"\n",
    "    fig2.savefig(heatmap_lag_png, dpi=200)\n",
    "    plt.close(fig2)\n",
    "    print(f\"Wrote {heatmap_lag_png}\")\n",
    "\n",
    "    # Bonus: scatter of best lag vs Chester horizon, filtered to strong correlations\n",
    "    strong = result_df[result_df[\"max_corr\"] > 0.4].copy()\n",
    "    if not strong.empty:\n",
    "        fig3, ax3 = plt.subplots(figsize=(8,4))\n",
    "        ax3.scatter(strong[\"CHS_horizon_hr\"]/24.0,\n",
    "                    strong[\"best_lag_days\"],\n",
    "                    s=40,\n",
    "                    alpha=0.7)\n",
    "        ax3.set_xlabel(\"Chester lead time (days)\")\n",
    "        ax3.set_ylabel(\"Best lag (days)\")\n",
    "        ax3.set_title(\"Lag vs Chester horizon (only pairs with corr > 0.4)\")\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        fig3.tight_layout()\n",
    "        scatter_png = out_dir / \"lag_corr_scatter_strong.png\"\n",
    "        fig3.savefig(scatter_png, dpi=200)\n",
    "        plt.close(fig3)\n",
    "        print(f\"Wrote {scatter_png}\")\n",
    "    else:\n",
    "        print(\"No strong correlation pairs above threshold, skip scatter plot.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LRF_RC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
