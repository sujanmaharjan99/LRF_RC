{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b642999a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/49] AL ...\n",
      "[2/49] AZ ...\n",
      "[3/49] AR ...\n",
      "[4/49] CA ...\n",
      "[5/49] CO ...\n",
      "[6/49] CT ...\n",
      "[7/49] DE ...\n",
      "[8/49] DC ...\n",
      "[9/49] FL ...\n",
      "[10/49] GA ...\n",
      "[11/49] ID ...\n",
      "[12/49] IL ...\n",
      "[13/49] IN ...\n",
      "[14/49] IA ...\n",
      "[15/49] KS ...\n",
      "[16/49] KY ...\n",
      "[17/49] LA ...\n",
      "[18/49] ME ...\n",
      "[19/49] MD ...\n",
      "[20/49] MA ...\n",
      "[21/49] MI ...\n",
      "[22/49] MN ...\n",
      "[23/49] MS ...\n",
      "[24/49] MO ...\n",
      "[25/49] MT ...\n",
      "[26/49] NE ...\n",
      "[27/49] NV ...\n",
      "[28/49] NH ...\n",
      "[29/49] NJ ...\n",
      "[30/49] NM ...\n",
      "[31/49] NY ...\n",
      "[32/49] NC ...\n",
      "[33/49] ND ...\n",
      "[34/49] OH ...\n",
      "[35/49] OK ...\n",
      "[36/49] OR ...\n",
      "[37/49] PA ...\n",
      "[38/49] RI ...\n",
      "[39/49] SC ...\n",
      "[40/49] SD ...\n",
      "[41/49] TN ...\n",
      "[42/49] TX ...\n",
      "[43/49] UT ...\n",
      "[44/49] VT ...\n",
      "[45/49] VA ...\n",
      "[46/49] WA ...\n",
      "[47/49] WV ...\n",
      "[48/49] WI ...\n",
      "[49/49] WY ...\n",
      "Writing 12564 rows to conus_usgs_discharge_stations.csv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Build a CSV of all USGS CONUS stream (ST) stations that measure discharge (00060),\n",
    "including drainage area (sq mi), latitude, and longitude.\n",
    "\n",
    "Requires: pip install dataretrieval pandas\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from dataretrieval import nwis\n",
    "\n",
    "# 48 states + DC (exclude AK, HI, territories)\n",
    "CONUS_STATES = [\n",
    "    \"AL\",\"AZ\",\"AR\",\"CA\",\"CO\",\"CT\",\"DE\",\"DC\",\"FL\",\"GA\",\"ID\",\"IL\",\"IN\",\"IA\",\"KS\",\"KY\",\n",
    "    \"LA\",\"ME\",\"MD\",\"MA\",\"MI\",\"MN\",\"MS\",\"MO\",\"MT\",\"NE\",\"NV\",\"NH\",\"NJ\",\"NM\",\"NY\",\"NC\",\n",
    "    \"ND\",\"OH\",\"OK\",\"OR\",\"PA\",\"RI\",\"SC\",\"SD\",\"TN\",\"TX\",\"UT\",\"VT\",\"VA\",\"WA\",\"WV\",\"WI\",\"WY\"\n",
    "]\n",
    "\n",
    "OUT_CSV = \"conus_usgs_discharge_stations.csv\"\n",
    "\n",
    "def fetch_state(state_cd: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query NWIS Site Service via dataretrieval for one state.\n",
    "    Filters:\n",
    "      - siteType = ST (streams)\n",
    "      - hasDataTypeCd=iv (instantaneous) to ensure parameterCd filter is valid\n",
    "      - parameterCd=00060 (discharge)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # dataretrieval passes args to the Site Service (RDB). This avoids the JSON 400s.\n",
    "        df, _md = nwis.what_sites(\n",
    "            stateCd=state_cd,\n",
    "            siteType=\"ST\",\n",
    "            hasDataTypeCd=\"iv\",\n",
    "            parameterCd=\"00060\",\n",
    "            seriesCatalogOutput=True,  # ensures parameterCd filter is supported\n",
    "        )\n",
    "        if df is None or df.empty:\n",
    "            return pd.DataFrame()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] {state_cd}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def col(df: pd.DataFrame, options: List[str]):\n",
    "    \"\"\"Return the first existing column from 'options', else None.\"\"\"\n",
    "    for c in options:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def normalize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Column names can vary slightly; resolve robustly\n",
    "    site_no_c   = col(df, [\"site_no\"])\n",
    "    agency_c    = col(df, [\"agency_cd\"])\n",
    "    name_c      = col(df, [\"station_nm\"])\n",
    "    state_c     = col(df, [\"state_cd\"])   # often numeric ANSI; fine for bookkeeping\n",
    "    site_type_c = col(df, [\"site_tp_cd\"])\n",
    "    huc_c       = col(df, [\"huc_cd\", \"hucCd\"])\n",
    "    lat_c       = col(df, [\"dec_lat_va\", \"lat\", \"latitude\"])\n",
    "    lon_c       = col(df, [\"dec_long_va\",\"lon\",\"longitude\"])\n",
    "    drain_c     = col(df, [\"drain_area_va\", \"contrib_drain_area_va\"])\n",
    "\n",
    "    keep = []\n",
    "    for _, r in df.iterrows():\n",
    "        keep.append({\n",
    "            \"agency_cd\": r.get(agency_c, None),\n",
    "            \"site_no\": r.get(site_no_c, None),\n",
    "            \"station_nm\": r.get(name_c, None),\n",
    "            \"state_cd\": r.get(state_c, None),\n",
    "            \"site_tp_cd\": r.get(site_type_c, None),\n",
    "            \"huc_cd\": r.get(huc_c, None),\n",
    "            \"lat\": pd.to_numeric(r.get(lat_c, None), errors=\"coerce\"),\n",
    "            \"lon\": pd.to_numeric(r.get(lon_c, None), errors=\"coerce\"),\n",
    "            \"drainage_area_sqmi\": pd.to_numeric(r.get(drain_c, None), errors=\"coerce\"),\n",
    "        })\n",
    "    out = pd.DataFrame(keep)\n",
    "\n",
    "    # Basic cleanup\n",
    "    out = out.dropna(subset=[\"site_no\"]).drop_duplicates(subset=[\"site_no\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def fill_missing_drainage(df: pd.DataFrame, batch_size: int = 200) -> pd.DataFrame:\n",
    "    \"\"\"For rows missing drainage area, bulk-fetch site info and fill 'drainage_area_sqmi'.\"\"\"\n",
    "    need = df[\"drainage_area_sqmi\"].isna()\n",
    "    if not need.any():\n",
    "        return df\n",
    "\n",
    "    sites = df.loc[need, \"site_no\"].astype(str).tolist()\n",
    "    filled = {}\n",
    "    for i in range(0, len(sites), batch_size):\n",
    "        chunk = sites[i:i+batch_size]\n",
    "        try:\n",
    "            info, _md = nwis.get_info(sites=\",\".join(chunk))\n",
    "            if info is None or info.empty:\n",
    "                continue\n",
    "            # get_info returns site metadata; look for drain_area_va\n",
    "            if \"site_no\" in info.columns:\n",
    "                info = info.drop_duplicates(subset=[\"site_no\"])\n",
    "                # column may be 'drain_area_va' (total) or have only contrib; prefer total\n",
    "                drain_c = \"drain_area_va\" if \"drain_area_va\" in info.columns else (\n",
    "                    \"contrib_drain_area_va\" if \"contrib_drain_area_va\" in info.columns else None\n",
    "                )\n",
    "                if drain_c:\n",
    "                    for _, r in info.iterrows():\n",
    "                        val = pd.to_numeric(r.get(drain_c, None), errors=\"coerce\")\n",
    "                        if pd.notna(val):\n",
    "                            filled[str(r[\"site_no\"])] = float(val)\n",
    "        except Exception as e:\n",
    "            print(f\"[INFO] fill chunk {i}-{i+batch_size}: {e}\")\n",
    "        time.sleep(0.05)  # be polite\n",
    "\n",
    "    if filled:\n",
    "        df.loc[need, \"drainage_area_sqmi\"] = df.loc[need, \"site_no\"].map(filled)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    frames = []\n",
    "    for i, st in enumerate(CONUS_STATES, 1):\n",
    "        print(f\"[{i}/{len(CONUS_STATES)}] {st} ...\")\n",
    "        raw = fetch_state(st)\n",
    "        norm = normalize(raw)\n",
    "        if not norm.empty:\n",
    "            frames.append(norm)\n",
    "        time.sleep(0.05)  # throttle a bit\n",
    "\n",
    "    if not frames:\n",
    "        print(\"No rows found.\")\n",
    "        pd.DataFrame(columns=[\n",
    "            \"agency_cd\",\"site_no\",\"station_nm\",\"state_cd\",\"site_tp_cd\",\"huc_cd\",\"lat\",\"lon\",\"drainage_area_sqmi\"\n",
    "        ]).to_csv(OUT_CSV, index=False)\n",
    "        print(f\"Saved empty CSV with headers to {OUT_CSV}\")\n",
    "        return\n",
    "\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "    # Keep only valid coordinates and stream sites (safety)\n",
    "    df = df[df[\"site_tp_cd\"] == \"ST\"]\n",
    "    df = df[df[\"lat\"].notna() & df[\"lon\"].notna()].copy()\n",
    "\n",
    "    # Try to fill missing drainage areas\n",
    "    df = fill_missing_drainage(df)\n",
    "    df[\"drainage_area_km2\"] = df[\"drainage_area_sqmi\"] * 2.58999\n",
    "    # Final ordering & write\n",
    "    cols = [\"agency_cd\",\"site_no\",\"station_nm\",\"state_cd\",\"site_tp_cd\",\"huc_cd\",\"lat\",\"lon\",\"drainage_area_km2\"]\n",
    "    df = df[cols].drop_duplicates(subset=[\"site_no\"]).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Writing {len(df)} rows to {OUT_CSV}\")\n",
    "    df.to_csv(OUT_CSV, index=False)\n",
    "    print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
